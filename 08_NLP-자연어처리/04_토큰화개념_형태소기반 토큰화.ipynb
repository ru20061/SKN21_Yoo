{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee251a30",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "\n",
    "토큰화(tokenization)는 자연어를 **모델이 이해할 수 있는 또는 모델이 다룰 수있는 기본 단위(Token)** 분할하는 과정을 말한다.   \n",
    "토큰으로 나누는 단위는 설계에 따라 문장, 어절, 형태소, 서브워드, 문자, 자모/알파벳 등 다양한 방식으로 나눌 수 있다.   \n",
    "- 예\n",
    "```bash\n",
    "원문: \"자연어 처리는 재미있다\"\n",
    "토큰화: [\"자연어\", \"처리\", \"는\", \"재미있다\"]\n",
    "```\n",
    "\n",
    "## 토큰화 방식\n",
    "- **단어 기반 토큰화(Word-Level Tokenization)**\n",
    "    - 어절(공백으로 구분) 또는 형태소 단위로 단어를 나누는 전통적인 방식이다.\n",
    "    - **한국어**는 교착어로 하나의 단어에 다양한 조사/어미가 결합된다. 그래서 어절단위로 토큰화할 경우 어휘사전의 크기가 기하급수적으로 늘어나는 문제가 있다.\n",
    "      - 예) \"학교\", \"학교가\", \"학교를\", \"학교에\", \"학교에서\", \"학교로\", \"학교의\", ...\n",
    "    - 이로 인해 미등록어휘(OOV - Out of Vocabulary)의 증가, 같은 의미를 가지는 단어들이 Vocab에 중복 등록, 메모리 낭비, 학습효율성 저하 등 다양한 문제가 생긴다.\n",
    "    - 그래서 **한국어의 경우 형태소 단위 토큰화**가 필요하다.\n",
    "\n",
    "- **서브워드 기반(Subword-level) — BPE, WordPiece, Unigram**\n",
    "    - Transformer 기반 모델(BERT, GPT, LLaMA 등)에서 표준으로 사용하는 방식.\n",
    "    - 단어를 기준으로 토큰화하지 않고 **문자(character)와 단어(word)의 중간 수준인 서브워드(subword) 단위로 토큰화**한다.\n",
    "    - **동작 원리**:\n",
    "        - 자주 등장하는 문자열 조합(서브워드)을 하나의 토큰으로 구성한다.\n",
    "        - 빈도가 높은 단어는 하나의 토큰으로, 빈도가 낮거나 희귀한 단어는 여러 서브워드로 분할한다.\n",
    "    - **예시**:\n",
    "        ```bash\n",
    "        입력: \"나는 밥을 먹었습니다. 나는 어제 밥을 했습니다.\"\n",
    "        \n",
    "        서브워드 토큰화 결과 (예시):\n",
    "        [\"나는\", \"밥\", \"을\", \"먹\", \"었\", \"습니다\", \".\", \"나는\", \"어제\", \"밥\", \"을\", \"하\", \"었\", \"습니다\", \".\"]\n",
    "        ```\n",
    "    - **장점**:\n",
    "        - **미등록 단어(OOV) 문제 해결**: 모든 단어를 서브워드 조합으로 표현 가능\n",
    "        - **어휘 사전 크기 최적화**: 단어 단위보다 작고, 문자 단위보다 효율적\n",
    "        - **다국어 지원**: 언어에 구애받지 않는 범용적 토큰화\n",
    "        - **형태론적 의미 포착**: 접두사, 접미사 등의 의미를 학습 가능\n",
    "    - **주요 알고리즘**:\n",
    "        - **BPE (Byte Pair Encoding)**: 가장 빈번한 연속 바이트/문자 쌍을 반복적으로 병합\n",
    "        - **WordPiece**: BERT에서 사용, BPE와 유사하지만 likelihood 기반으로 병합\n",
    "        - **Unigram**: 확률 모델 기반으로 최적의 서브워드 분할 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1a834",
   "metadata": {},
   "source": [
    "# 한국어 형태소 분석기\n",
    "\n",
    "- kiwipiepy와 konlpy 는 대표적인 한국어 형태소 분석기이다.\n",
    "\n",
    "## kiwipiepy\n",
    "**kiwipiepy**는 C++로 구현된 한국어 형태소 분석기 Kiwi(Korean Intelligent Word Identifier)를 Python 환경에서 사용할 수 있도록 한 라이브러리이다. \n",
    "\n",
    "- 빠른 속도  \n",
    "- 최신 품사 체계 지원  \n",
    "- 사용자 사전 확장 용이  \n",
    "- 최근 가장 널리 쓰이는 한국어 토크나이저 중 하나이다.\n",
    "- https://github.com/bab2min/kiwipiepy\n",
    "  \n",
    "### 설치 방법\n",
    "\n",
    "```bash\n",
    "pip install kiwipiepy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b796423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uv pip install ipykernel ipywidgets\n",
    "import kiwipiepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87abcc",
   "metadata": {},
   "source": [
    "### 주요 클래스 및 함수\n",
    "\n",
    "#### Kiwi 클래스\n",
    "- Kiwi의 핵심 클래스이며, 형태소 분석과 토큰화 기능을 모두 제공한다.\n",
    "- Kiwi 품사는 세종 말뭉치를 기반으로 한다.\n",
    "  - 품사 시작 글자\n",
    "  - 체언(명사, 대명사): `N`, 용언(동사, 형용사): `V`, 수식언(관형사, 부사): `M`,  관계언(조사):`J`, 어미: `E`, 기호: `S`\n",
    "    - https://github.com/bab2min/kiwipiepy?tab=readme-ov-file#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8\n",
    "- 메소드\n",
    "  - `tokenzie(text)`: 형태소 분석 기반 토큰화 수행\n",
    "  - `analyze(text)`: tokenize보다 좀 더 상세한 분석을 진행한다. 여러 분석결과를 조회할 수있다.\n",
    "  - `add_user_word(word, pos, score)`: 사전에 직접 단어 등록\n",
    "  - `space(text)`: 띄어 쓰기 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27aea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip uninstall kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8876a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install kiwipiepy==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd597bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from pprint import pprint #자료구조를 출력을 보기좋게 print 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 토큰화 = tokenize()\n",
    "#######################\n",
    "\n",
    "text = '나는 자연어 처리를 공부한다. \\n내일은 뭘 공부할까?'\n",
    "tokens = kiwi.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d348d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    r = f'토큰문자열 : {token.form} , 원형(lemma) : {token.lemma}, 품사(tag): {token.tag}\\\n",
    "        시작위치: {token.start}, 글자수: {token.len}, 토큰이 있는 행번호: {token.line_number}\\\n",
    "        몇번째 문장에 있는지: {token.sent_position}, 문장에서 몇번째 어절인지: {token.word_position}'\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여러 문서를 토큰화 할 때는 list로 묶어서 전달.\n",
    "text_list = ['나는 자연어 처리를 공부한다.','내일은 뭘 공부할까?']\n",
    "\n",
    "tokens = kiwi.tokenize(text_list)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#여러 문서를 토큰화 할 때는 list로 묶어서 전달.\n",
    "text = ['나는 자연어 처리를 공부한다.','자연어 처리는 nlp 라고 한다.','내일은 뭘 공부할까?']\n",
    "\n",
    "result = kiwi.analyze(text , top_n=2)\n",
    "\n",
    "for token in result:\n",
    "    pprint(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 띄어쓰기 교정 = space()\n",
    "#######################\n",
    "\n",
    "text = '자연어처리는재미있는분야이다.또재미있는것은무가있을까?'\n",
    "result = kiwi.space(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['자연어처리는재미있는분야이다.또재미있는것은무가있을까?','아버지가방에들어가신다']\n",
    "result = kiwi.space(text_list)\n",
    "for token in result:\n",
    "    pprint(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 문장분리 = split_into_sents()\n",
    "#######################\n",
    "txt='특히 양현준은 팀이 0-1로 뒤진 전반 31분 귀중한 동점골을 터뜨렸다.하타테가 올린 크로스를 오른발 슈팅으로 연결해 페예노르트 골문을 연 열었다.오현규와 양현준은 득점, 설영우는 도움을 올렸다.'\n",
    "result=kiwi.split_into_sents(txt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890aa913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in result:\n",
    "    pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c37e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"어제 친구가 '집에가고 싶다.'라고 이야기 했다.\"\n",
    "result = kiwi.split_into_sents(txt)\n",
    "for sent in result:\n",
    "    pprint(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = result[0]\n",
    "print(sent.text)\n",
    "\n",
    "if sent.subs != None:\n",
    "    print(sent.subs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 사전에 사용자단어 추가 = add_user_word(단어,품사,score)\n",
    "# score : 토큰화할 때 그 단어의 운선순위를 조절하는 가중치 값. 클 수록 더 선호하게 된다.\n",
    "# 0 : 중립 , 고유명사들은 0을 지정한다.\n",
    "# - 딥러닝 : 5 , 딥 : 10 , 러닝: 10 -> 딥,러닝 으로 나누는 것을 더 선호하게 된다.\n",
    "#######################\n",
    "text = '박새로이가 왔다'\n",
    "kiwi.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi.add_user_word('박새로이','NP',0)\n",
    "kiwi.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 불용어(Stop words) 처리 - 토큰화 했을 떄 제거할 토큰(단어)들. \n",
    "#######################\n",
    "\n",
    "# kiwi 제공하는 불용어\n",
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "sw = Stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 조회\n",
    "# sw.stopwords\n",
    "len(sw.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab4736",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '나는 자연어 처리를 공부한다. 자연어처리는 NLP라고 한다.'\n",
    "result = kiwi.tokenize(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11be6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords 들 제거 - stopwords객체.filter(토큰list)\n",
    "result2 = sw.filter(result)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 추가/삭제\n",
    "#sw.add(('단어','품사tag')) # 품사 tag 생략 -> NNP(고유명사)\n",
    "# sw.remove(('단어','품사tag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd92772",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kiwi.tokenize('이름이 박새로이입니다.')\n",
    "print(result)\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.add(('박새로이','NP'))\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3350c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.remove(('박새로이','NP'))\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00248452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출\n",
    "result = kiwi.tokenize('나는 NLP 공부를 어제부터 시작했습니다.')\n",
    "token_list=[]\n",
    "for token in result:\n",
    "    if token.tag.startswith('N'): #명사는 N으로 시작.\n",
    "        token_list.append(token)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aee9d",
   "metadata": {},
   "source": [
    "## KoNLPy(코엔엘파이)\n",
    "- KoNLPY는 한국어 자연어 처리(Natural Language Processing) 파이썬 라이브러리이다.  한국어 처리를 위한 tokenize, 형태소 분석, 어간추출, 품사부착(POS Tagging) 등의 기능을 제공한다. \n",
    "- http://KoNLPy.org/ko/latest/\n",
    "- 기존의 개발된 다양한 형태소 분석기를 통합해서 동일한 interface로 호출 할 수 있게 해준다.\n",
    "\n",
    "### KoNLPy 설치\n",
    "- 설치 순서\n",
    "  1. Java 실행환경 설치\n",
    "  2. JPype1 설치\n",
    "  3. koNLPy 설치\n",
    "\n",
    "1. **Java 설치**\n",
    "  - https://www.oracle.com/java/technologies/downloads/\n",
    "    - OS에 맞는 설치 버전을 다운받아 설치한다.\n",
    "    - MAC: ARM일 경우: **ARM64 CPU** - ARM64 DMG Installer, **Intel CPU**: x64 DMG Installer\n",
    "  - 시스템 환경변수 설정\n",
    "      - `JAVA_HOME` : 설치 경로 지정\n",
    "      - `Path` : `설치경로\\bin` 경로 지정\n",
    "\n",
    "2. **JPype1 설치**\n",
    "   - 파이썬에서 자바 모듈을 호출하기 위한 연동 패키지\n",
    "   - 설치: `pip install JPype1`\n",
    "\n",
    "3. **KoNLPy 설치**\n",
    "- `pip install konlpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3046bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c7d17",
   "metadata": {},
   "source": [
    "### 형태소 분석기/사전\n",
    "- 형태소 사전을 내장하고 있으며 형태소 분석 함수들을 제공하는 모듈\n",
    "\n",
    "#### KoNLPy 제공 형태소 분석기\n",
    "- Open Korean Text\n",
    "    - 트위터에서 개발\n",
    "    - https://github.com/open-korean-text/open-korean-text\n",
    "- Hannanum(한나눔)\n",
    "    - KAIST Semantic Web Research Center 에서 개발\n",
    "    - http://semanticweb.kaist.ac.kr/hannanum/\n",
    "- Kkma(꼬꼬마)\n",
    "    - 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.\n",
    "    - http://kkma.snu.ac.kr/\n",
    "- Komoran(코모란)\n",
    "    - Shineware에서 개발.\n",
    "    - 오픈소스버전과 유료버전이 있음\n",
    "    - https://github.com/shin285/KOMORAN\n",
    "- Mecab(메카브) \n",
    "    - 일본어용 형태소 분석기를 한국에서 사용할 수 있도록 수정\n",
    "    - windows에서는 설치가 안됨\n",
    "    - https://bitbucket.org/eunjeon/mecab-ko\n",
    "\n",
    "\n",
    "### 형태소 분석기 공통 메소드\n",
    "- `morphs(string)` : 형태소 단위로 토큰화(tokenize)\n",
    "- `nouns(string)` : 명사만 추출하여 토큰화(tokenize)    \n",
    "- `pos(string)`: 품사 부착\n",
    "    - 형태소 분석기 마다 사용하는 품사태그가 다르다.\n",
    "        - https://konlpy-ko.readthedocs.io/ko/v0.5.2/morph/\n",
    "- `tagset`: 형태소 분석기가 사용하는 품사태그 설명하는 속성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a9426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a80e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd51d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d66f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26e26ee",
   "metadata": {},
   "source": [
    "# WordCloud\n",
    "\n",
    "WordCloud는 텍스트 데이터에서 단어의 등장 빈도를 시각적으로 표현한 그래픽이다.\n",
    "- 특징\n",
    "  - 자주 등장하는 단어일수록 글자 크기가 커진다.\n",
    "  - 텍스트 전체의 주제를 직관적으로 파악할 수 있다.\n",
    "  - 문서의 핵심키워드를 빠르게 파악할 수 있어 텍스트 분석에서 탐색적 데이터 분석(EDA) 단계에서 자주 활용된다.\n",
    "- 설치\n",
    "  - `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b8a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088404c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9932a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81bd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
