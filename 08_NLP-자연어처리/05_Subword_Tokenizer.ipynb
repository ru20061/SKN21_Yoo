{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 형태소 분석 기반 토큰화의 문제\n",
    "- 형태소 분석기는 작성된 알고리즘 또는 학습된 내용을 바탕으로 토큰화를 하기 때문에 오탈자나 띄어쓰기 실수, 신조어, 외래어, 고유어 등이 사용된 경우 제대로 토큰화 하지 못한다.\n",
    "- 그래서 발생 할 수있는 잠재적 문제점\n",
    "    - 어휘사전을 크게 만든다.\n",
    "        - 같은 의미의 단어가 형태소 분석이 안되어 여러개 등록될 수있다.\n",
    "        - ex) 신조어 `돈쭐` 이라는 단어를 인식 못할 경우 `\"돈쭐내러\", \"돈쭐나\", \"돈쭐냄\"` 등이 다 등록 될 수 있다.\n",
    "    - OOV(Out Of Vocab)에 대응하기 어렵게 만든다.\n",
    "        - 같은 어근의 단어가 있지만 조사등이 바뀐 신조어등을 OOV로 인식할 수있다.\n",
    "\n",
    "\n",
    "## 어휘 사전(Vocabulary)과 Out Of Vocabulary (OOV)\n",
    "\n",
    "- 어휘사전(Vocab)은 토크나이저(Tokenizer)가 사용하는 모든 토큰의 집합이며,**각 토큰**을 고유한 **정수 ID**에 매핑한 사전이다. 토크나이저가 텍스트를 토큰 ID 시퀀스로 변환할 때 기준으로 사용된다. \n",
    "\n",
    "   - 매핑된 정수는 모델에 입력되는 텍스트 데이터를 숫자 형식으로 변환해 모델이 처리할 수 있도록 돕는다.\n",
    "   - 예\n",
    "        ```bash\n",
    "        {\n",
    "            \"자연어\": 0,\n",
    "            \"처리\": 1,\n",
    "            \"는\": 2,\n",
    "            \"재미있다\": 3,\n",
    "            \"공부\": 4,\n",
    "        }\n",
    "        ```\n",
    "- **Out Of Vocabulary (OOV)**\n",
    "   - 어휘 사전(Vocab): 코퍼스를 구성하는 모든 토큰의 집합.\n",
    "   - **OOV**란 어휘 사전에 포함되지 않은 토큰을 의미하며, 모델이 해당 토큰을 처리할 수 없기 때문에 일반적으로 특별한 토큰(예: `[UNK]`)으로 대체되거나 다른 방식으로 처리된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Tokenization(하위 단어 토큰화)\n",
    "\n",
    "## 정의\n",
    "\n",
    "- Subword Tokenization은 단어를 더 작은 단위(subword)로 나누어 텍스트를 토큰화하는 방식이다.  \n",
    "    - subword는 하나의 단어를 구성하는 단어들을 말한다.(coworker: co, work, er)\n",
    "- 주로 자주 등장하는 단어의 일부를 공통된 토큰으로 만들고, 희귀하거나 복합적인 단어는 작은 조각(subword)으로 나누어 처리한다.\n",
    "- 단어 자체를 그대로 사용하기보다는 단어의 일부를 나누어 처리함으로써 새로운 단어나 미등록 단어(Out-of-Vocabulary) 문제를 줄일 수 있다.\n",
    "\n",
    "## 장점\n",
    "\n",
    "1. **미등록 단어 처리 가능**  \n",
    "   -  새로운 단어(신조어, 속어, 고유어등)가 등장해도 미리 정의된 subword를 조합해서 표현할 수 있어 OOV 문제를 줄일 수 있다.  \n",
    "\n",
    "2. **어휘 크기 축소**  \n",
    "   - 같은 subword를 여러 단어에서 공유함으로써, 완전한 단어를 사용하는 경우보다 어휘집의 크기를 작게 유지할 수 있다.\n",
    "\n",
    "\n",
    "## 종류\n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**  \n",
    "   - 자주 등장하는 문자 쌍을 반복적으로 병합해 서브워드를 생성하는 방식.\n",
    "   - OpenAI의 GPT 모델에 사용된 토크나이저이다.\n",
    "\n",
    "2. **Unigram**  \n",
    "   - 빈도기반 확률모델에 따라 subword 단위를 선택하는 방식이다.  \n",
    "   - BPE보다 유연하여 더 다양한 분할 결과를 얻을 수 있다.\n",
    "\n",
    "3. **WordPiece**  \n",
    "   - BPE와 유사하지만, 빈도수가 아니라, 가능성이 높은 조합(합쳐질 가능성이 높은 subword)에 기반해 subword들을 찾는다.\n",
    "   - Google의 BERT 모델에 사용된 토크나이저이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding 방식\n",
    "\n",
    "- 원래 Text data 압축을 위해 만들어진 방법으로 text 에서 자주 같이 나오는 두 글짜 쌍을 합쳐서 하나의 부호(기호,글자)로 만들어 나가면서 글자 수를 줄이는 알고리즘이다. \n",
    "- 연속된 글자 쌍이 더 나타나지 않거나 정해진 어휘사전 크기에 도달 할 때 까지 조합을 찾아 부호화 하는 작업을 반복한다.\n",
    "\n",
    "## text 압축 방식의 예\n",
    "- 원문: abracadabra\n",
    "1. AracadAra: ab -> A :=> 원문에서 가장 빈도수 많은 ab를 A(부호로 아무 글자나 사용할 수 있다.)로 치환\n",
    "2. ABcadAB: ra -> B :=> 1에서 가장 빈도수가 많은 ra를 B로 치환\n",
    "3. CcadC: AB -> C :=> 2에서 가장 빈도수 맣은 AB를 C로 치환한다.(치환된 글자 쌍도 변환대상에 포함된다.)\n",
    "\n",
    "## BPE Tokenizer 방식\n",
    "BPE 토크나이저는 자주 등장하는 글자 쌍을 찾아 치환하는 대신 **단어 사전**에 추가한다.\n",
    "\n",
    "### 예)\n",
    "1. 말뭉치의 토큰들의 빈도수, 어휘사전은 아래와 같을 경우\n",
    "    - 빈도사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
    "    - 어휘사전: ['low', 'lower', 'newest', 'widest']\n",
    "2. 빈도 사전내의 모든 단어들을 글자 단위로 나눈다. (Pre Tokenization)\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "3. 빈도 사전을 기준으로 가장 자주 등장하는 글자 쌍(byte pair)를 찾는다.  위에서는 **'e'와 's'가 총 9번으로 가장 많이 등장함**. 'e'와 's'를 'es'로 합치고 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'es'**, 't', 6), ('w', 'i', 'd', **'es'**, 't', 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**]\n",
    "4. 3 번의 과정을 계속 반복한다. 빈도수가 가장 많은 'es'와 't' 쌍을 'est'로 병합하고 'est'를 어휘 사전에 추가한다.\n",
    "    - 빈도사전: ('l', 'o', 'w',  5), ('l', 'o', 'w', 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**]\n",
    "5. 만약 10번 반복했다고 하면 다음과 같은 빈도 사전과 어휘 사전이 생성된다.\n",
    "    - 빈도 사전: (**'low'**, 5), (**'low'**, 'e', 'r', 2), ('n', 'e', 'w', **'est'**, 6), ('w', 'i', 'd', **'est'**, 3)\n",
    "    - 어휘사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'es'**, **'est'**, **'lo'**,**'low'**, **'low'**, **'ne'**, **'new'**, **'newest'**, **'wi'**, **'wid'**, **'widest'**]\n",
    "\n",
    "- 위와 같이 어휘 사전이 만들어 지면 원래 어휘서전에 없던 것들에 대한 처리를 할 수있다.\n",
    "    - ex)\n",
    "        - 'newer' :=> 'new', 'e', 'r', \n",
    "        - 'lowest' :=> 'low', 'est'\n",
    "        - 'wider' :=> 'wid', 'e', 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece tokenizer\n",
    "\n",
    "- Byte Pair Encoding 이 빈도 기반이라면 wordpiece tokenizer는 확률 기반으로 글자 쌍을 병합한다.\n",
    "- 두개 글자 쌍의 빈도수를 각 개별 글자 빈도수의 곱으로 나눈 점수가 가장 높은 순서대로 글자쌍을 묶어 나간다.\n",
    "\n",
    "$$\n",
    "score = \\cfrac{f(x, y)}{f(x)\\cdot f(y)} \n",
    "$$\n",
    "\n",
    "함수 f는 빈도를 나타내며 x, y는 병합하려는 하위 단어이다.\n",
    "\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w')\n",
    "- 가장 빈도수가 높은 쌍은 'e','s'로 9번 등장한다. 이때 각 글자는 전체에서 각각 'e'는 17번, 's'는 9번 등장한다. 위 공식에 대입하면 score는 $\\frac{9}{17 \\times 9} \\approx 0.06$ 이다.\n",
    "- 'i'와 'd' 쌍은 3번만 등장하지만 전체에서 각각 'i' 3번, 'd' 3번 등장한다. 그래서 score는 $\\frac{3}{3 \\times 3} \\approx 0.33$ 이다.\n",
    "- 나타난 빈도수는 'es' 가 많치만 더 높은 score를 가지는 'id' 쌍을 병합한다.\n",
    "- 빈도사전: ('l','o','w', 5), ('l','o','w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', **'id'**, 'e', 's', 't', 3)\n",
    "- 어휘사전: ('d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', **'id'**)\n",
    "위의 작업을 반복해 연속된 글자 쌍이 더이상 나타나지 않거나 어휘 사전 max 크기에 도달할 때 까지 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram 방식\n",
    "- 빈도 기반 확률 모델을 사용하여 효율적으로 서브워드를 선택하고, 불필요한 서브워드를 제거해 최적의 어휘 크기를 찾는 알고리즘\n",
    "\n",
    "\n",
    "- **초기 어휘 집합 구성**\n",
    "    - 대상 text에 모든 단어와 그 서브스트링을 포함한 어휘 집합을 생성한다. 이 어휘 집합은 나올 수있는 모든 subword들을 다 모아놓은 것이다. \n",
    "    - 예를 들어 \"hug\" 단어의  [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"hug\"]  substring을 만든다. 이들이 subword 후보가 된다.\n",
    "- **각 Subword의 빈도수 기반 확률 계산**\n",
    "    -  $\\cfrac{subword가\\;나타난\\;횟수}{전체\\;빈도수}$ 로 각 subword들의 나타난 확률을 계산한다.\n",
    "- **가능한 분할에 대한 확률 계산**\n",
    "    - 단어를 여러 서브워드로 분할할 수 있는 경우, 각 분할에 대한 전체 확률을 계산한다.\n",
    "    - 확률 계산은 $ P(subword1)\\;\\times \\; P(subword2)\\;\\times\\; ..$ 으로 계산한다.\n",
    "    - 예를 들어 \"hug\" 를 분할 한다고 했을 때\n",
    "        1. \\[\"h\", \"u\", \"g\"\\]: $ P(h) \\times P(u) \\times P(g) $\n",
    "        2. \\[\"hu\", \"g\"\\]: $ P(pu) \\times P(g) $\n",
    "\n",
    "   - 각각의 확률을 계산한 후, **가장 높은 확률**을 가진 분할을 선택한다.\n",
    "     - 위 예에서 만약 1의 확률이 0.01 이고 2의 확률이 0.00001 이라면 첫번째 분할이 선택된다.\n",
    "\n",
    "- **서브워드 제거**\n",
    "    - 위의 훈련 과정에서 불필요한 서브워드를 제거하면서 최적의 어휘 집합을 찾아간다. \n",
    "    - 제거 대상은 빈도수가 낮거나 조합에 크게 영향을 주지 않은 subword들이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### korpora 말뭉치\n",
    "> - 다양한 한글 데이터셋을 제공하는 패키지\n",
    "> - `pip install korpora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m11 packages\u001b[0m \u001b[2min 359ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m7 packages\u001b[0m \u001b[2min 97ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 376ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdataclasses\u001b[0m\u001b[2m==0.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkorpora\u001b[0m\u001b[2m==0.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxlrd\u001b[0m\u001b[2m==2.0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 19.0MB/s]\n",
      "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 28.6MB/s]                            \n",
      "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 39.3MB/s]                            \n",
      "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 58.4MB/s]                            \n",
      "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 58.6MB/s]                            \n",
      "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 87.7MB/s]                            \n",
      "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 83.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-03: 34.3MB [00:00, 36.5MB/s]                            \n",
      "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 50.4MB/s]                            \n",
      "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 54.1MB/s]                            \n",
      "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 66.7MB/s]                            \n",
      "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 97.0MB/s]                            \n",
      "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 72.4MB/s]                            \n",
      "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 59.4MB/s]                            \n",
      "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 75.4MB/s]                            \n",
      "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 46.9MB/s]                            \n",
      "[korean_petitions] download petitions_2018-12: 33.0MB [00:01, 25.3MB/s]                            \n",
      "[korean_petitions] download petitions_2019-01: 34.8MB [00:01, 22.4MB/s]                            \n",
      "[korean_petitions] download petitions_2019-02: 30.8MB [00:01, 29.6MB/s]                            \n",
      "[korean_petitions] download petitions_2019-03: 34.9MB [00:01, 22.2MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Korpora.korpus_korean_petitions.KoreanPetitionsKorpus at 0x1b6cdb7d550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "corpus = Korpora.load('korean_petitions')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 433631)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions = corpus.get_all_texts()\n",
    "type(petitions) , len(petitions) # 청와대청원 데이터셋. list[str] str:개별 청원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장\n",
    "import os\n",
    "os.makedirs('data/petitions',exist_ok=True)\n",
    "with open('data/petitions/petition_corpus.txt','wt',encoding='utf-8') as fw:\n",
    "    for p in petitions:\n",
    "        fw.write(p+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드 : 문자열로 load\n",
    "with open('data/petitions/petition_corpus.txt','rt',encoding='utf-8') as fr:\n",
    "    petitions_txt = fr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용,\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petitions_txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face tokenizers 패키지 사용해 토큰화 수행\n",
    "\n",
    "## 주요 라이브러리 \n",
    "- [tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "    - huggingface에서 개발한 tokenizer 라이브러리로 BPE, WordPiece, Unigram 알고리즘을 지원한다. \n",
    "    - 설치: `pip install tokenizers`\n",
    "- [Sentencepiece](https://github.com/google/sentencepiece)\n",
    "    - 구글에서 개발한 subword tokenizer 라이브러리로 BPE, Unigram 방식 지원.\n",
    "    - 설치: `pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face tokenizers 패키지이용\n",
    "- Hugging face는 \n",
    "  - Subword방식의 토크나이저를 생성할 수 있는 알고리즘을 제공한다. `tokenizers` library를 이용해 사용할 수 있다.\n",
    "  - GPT, BERT등 다양한 LLM 모델에서 사용된 Pretrained Tokenizer들을 제공한다. `transformers` Library를 이용해 사용할 수 있다.\n",
    "- 설치: `pip install tokenizers`\n",
    "- Tokenizer 생성\n",
    "    - 토큰화 알고리즘을 지정해 instance 생성.\n",
    "- Trainer 생성\n",
    "    - 학습 파라미터를 설정해서 instance 생성\n",
    "- Tokenizer 학습\n",
    "    - train() 메소드: 학습 text 파일 경로를 지정해서 학습\n",
    "    - train_from_iterator() 메소드: 학습할 string들을 iterator를 통해 제공.\n",
    "- https://github.com/huggingface/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Tokenizer (어휘사전을 바탕으로 토큰화를 처리하는 객체.)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "#Pre Tokenizer 로 Subword 방식 토큰화 전이 미리 토큰화를 처리하는 방식을 지정.\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "#Trainer (토크나이저 모델 학습기) - 알고리즘에 맞춰 trainer를 제공.\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 객체 생성\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[unk]') # unknown token(모르는 단어를 표현할 토큰)을 설정\n",
    ")\n",
    "\n",
    "#토크나이저에 pre tokenizer 설정\n",
    "## BPE 알고리즘으로 토큰화 하기 전에 적용할 나누는 방법 - Whitespace(): 공백 기준으로 미리 나눈다.\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer 생성 - 학습 하이퍼 파라미터들을 설정\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000, # 어휘 사전의 최대크기.\n",
    "    min_frequency=10, # 최소 출연 횟수. 지정한 횟수(10) 이하로 나온 쌍(pair)는 단어사전에서 제외.\n",
    "    special_tokens=['[UNK]','[PAD]'], # 어휘 사전에 추가할 특수목적 토큰. unk_token은 반드시 추가한다.\n",
    "    continuing_subword_prefix='##' \n",
    "    # # 단어의 시작이 아니라 연결 subword의 앞에 붙이는 prefix\n",
    "    # BpeTrainer default : 안붙인다. ('')\n",
    "    # WordPieceTrainter : '##'이 default\n",
    "    # UnigramTrainer : ''이 default. 안붙인다.\n",
    ")\n",
    "\n",
    "# Special Token : \n",
    "# -특수목적 토큰. \n",
    "# -토크나이저나 모델에서 사용할 특정 목적의 토큰으로 우리가 직접 어휘사전에 추가해야 한다.\n",
    "# 대표적인 special token들. 보통 [] 나 <> 로 묶어준다.\n",
    "# - OOV 표시 토큰 : <unk>,[unk]로 표현\n",
    "# - Padding 토큰 : <pad>로 표현. 글자 수를 맞춰주기 위해 글자수가 적은 문장에 추가하는 토큰\n",
    "# - 문서의 시작 : <sos>로 표현\n",
    "# - 문서의 긑 : <eos>\n",
    "# - <cls> : 문서의 시작 + 전체 문서의 의미를 표현 (BERT 모델이 사용하는 특수토큰.)\n",
    "# - 문장내 일부 토큰을 가리기: <mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린 시간 : 29.697391510009766 초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "s = time.time()\n",
    "\n",
    "tokenizer.train(['data/petitions/petition_corpus.txt'],trainer=trainer)\n",
    "\n",
    "print('학습에 걸린 시간 :',time.time()-s, '초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 파일로 저장 -> 어휘사전 + 설정들이 저장.(json)\n",
    "os.makedirs('saved_models',exist_ok=True)\n",
    "save_path = 'saved_models/petitions_bpe.json'\n",
    "tokenizer.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 토크나이저 불러오기\n",
    "from tokenizers import Tokenizer\n",
    "load_tokenizer = Tokenizer.from_file(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size (토큰(어휘) 개수)\n",
    "tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab 조회\n",
    "#tokenizer.get_vocab() # 딕셔너리로 반환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문장\n",
    "sports_txt = \"프리미어리그 역대 개인 최다골 기록을 보유하고 있는 시어러가 손흥민의 골 결정력을 재차 극찬했다.\"\n",
    "petition_txt = \"이 글을 쓴 이유는 다름아닌 '전안법'시행 반대를 주장하기 위해서입니다. 먼저, '전안법'은 전기용품 및 생활용품을 판매하는 업체에서 KC인증마크를 의무적으로 받는 것입니다.\"\n",
    "comment_txt = \"멋진 식사를 즐기기에 좋은 장소 - 채식 메뉴가 정말 훌륭했습니다. 당근 케이크는 아마도 내가 먹어본 디저트 중 최고였을 거예요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화 : text(str) - > token들\n",
    "token_out = tokenizer.encode(sports_txt)\n",
    "token_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '리',\n",
       " '미',\n",
       " '어',\n",
       " '리',\n",
       " '그',\n",
       " '역',\n",
       " '대',\n",
       " '개인',\n",
       " '최',\n",
       " '다',\n",
       " '골',\n",
       " '기록',\n",
       " '을',\n",
       " '보유',\n",
       " '하고',\n",
       " '있는',\n",
       " '시',\n",
       " '어',\n",
       " '러',\n",
       " '가',\n",
       " '손',\n",
       " '흥',\n",
       " '민',\n",
       " '의',\n",
       " '골',\n",
       " '결정',\n",
       " '력을',\n",
       " '재',\n",
       " '차',\n",
       " '극',\n",
       " '찬',\n",
       " '했다',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding(토큰화 결과) 에서 각 토큰들을 문자열로 조회\n",
    "token_out.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7390,\n",
       " 5123,\n",
       " 5330,\n",
       " 6140,\n",
       " 5123,\n",
       " 4128,\n",
       " 6180,\n",
       " 4589,\n",
       " 8414,\n",
       " 6891,\n",
       " 4563,\n",
       " 4027,\n",
       " 9449,\n",
       " 6364,\n",
       " 9442,\n",
       " 8209,\n",
       " 8219,\n",
       " 5908,\n",
       " 6140,\n",
       " 4980,\n",
       " 3902,\n",
       " 5802,\n",
       " 7638,\n",
       " 5332,\n",
       " 6384,\n",
       " 4027,\n",
       " 8940,\n",
       " 8644,\n",
       " 6449,\n",
       " 6793,\n",
       " 4129,\n",
       " 6795,\n",
       " 8565,\n",
       " 15]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#각 토큰들의 id(정수)로 반환.\n",
    "token_out.ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '글을', '쓴', '이유는', '다', '름', '아닌', \"'\", '전', '안', '법', \"'\", '시행', '반대', '를', '주장', '하기', '위해서', '입니다', '.', '먼저', ',', \"'\", '전', '안', '법', \"'\", '은', '전기', '용', '품', '및', '생활', '용', '품', '을', '판매', '하는', '업체', '에서', 'K', 'C', '인', '증', '마', '크', '를', '의무', '적으로', '받는', '것입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "token_out2 = tokenizer.encode(petition_txt)\n",
    "print(token_out2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6396,\n",
       " 8577,\n",
       " 6044,\n",
       " 9246,\n",
       " 4563,\n",
       " 5107,\n",
       " 8323,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 8656,\n",
       " 8658,\n",
       " 5101,\n",
       " 8953,\n",
       " 8301,\n",
       " 8580,\n",
       " 8212,\n",
       " 15,\n",
       " 8855,\n",
       " 13,\n",
       " 8,\n",
       " 6476,\n",
       " 6073,\n",
       " 5412,\n",
       " 8,\n",
       " 6360,\n",
       " 9353,\n",
       " 6278,\n",
       " 7375,\n",
       " 5345,\n",
       " 8437,\n",
       " 6278,\n",
       " 7375,\n",
       " 6364,\n",
       " 9118,\n",
       " 8208,\n",
       " 8692,\n",
       " 8210,\n",
       " 44,\n",
       " 36,\n",
       " 6400,\n",
       " 6625,\n",
       " 5140,\n",
       " 7091,\n",
       " 5101,\n",
       " 8521,\n",
       " 8253,\n",
       " 8546,\n",
       " 8299,\n",
       " 15]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_out2.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9246"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 토큰 문자열 -> 토큰 ID\n",
    "tokenizer.token_to_id('이유는')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이유는'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 ID -> 토큰 문자열\n",
    "tokenizer.id_to_token(9246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이 글을 쓴 이유는 다 름 아닌 ' 전 안 법 ' 시행 반대 를 주장 하기 위해서 입니다 . 먼저 , ' 전 안 법 ' 은 전기 용 품 및 생활 용 품 을 판매 하는 업체 에서 K C 인 증 마 크 를 의무 적으로 받는 것입니다 .\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 id 리스트 -> 문장(str)\n",
    "decode_output = tokenizer.decode(token_out2.ids)\n",
    "decode_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"이글을쓴이유는다름아닌'전안법'시행반대를주장하기위해서입니다.먼저,'전안법'은전기용품및생활용품을판매하는업체에서KC인증마크를의무적으로받는것입니다.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token_out2.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordpiece 방식으로 학습\n",
    "from tokenizers.models import WordPiece # , BPE\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "# 토크나이저 생성\n",
    "wp_tokenizer = Tokenizer(\n",
    "    WordPiece(unk_token='<unk>')\n",
    ")\n",
    "\n",
    "# pre tokenizer 설정\n",
    "wp_tokenizer.pre_tokenizer=Whitespace()\n",
    "\n",
    "# trainer 설정\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=20000,\n",
    "    special_tokens=['<unk>','<pad>','<eos>','<sos>','<mask>','<sep>']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린 시간 : 52.44146680831909 초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "s = time.time()\n",
    "\n",
    "wp_tokenizer.train(['data/petitions/petition_corpus.txt'],trainer=trainer)\n",
    "\n",
    "print('학습에 걸린 시간 :',time.time()-s, '초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizer.save('saved_models/petitions_wordpiece.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_wp_tokenizer = Tokenizer.from_file('saved_models/petitions_wordpiece.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'彗'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.id_to_token(2000) # id -> 토큰문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.token_to_id('彗') # 토큰문자열 -> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=34, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코딩\n",
    "encoding  = wp_tokenizer.encode(sports_txt)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프',\n",
       " '##리',\n",
       " '##미',\n",
       " '##어',\n",
       " '##리',\n",
       " '##그',\n",
       " '역',\n",
       " '##대',\n",
       " '개인',\n",
       " '최',\n",
       " '##다',\n",
       " '##골',\n",
       " '기록',\n",
       " '##을',\n",
       " '보유',\n",
       " '##하고',\n",
       " '있는',\n",
       " '시',\n",
       " '##어',\n",
       " '##러',\n",
       " '##가',\n",
       " '손',\n",
       " '##흥',\n",
       " '##민',\n",
       " '##의',\n",
       " '골',\n",
       " '결정',\n",
       " '##력을',\n",
       " '재',\n",
       " '##차',\n",
       " '극',\n",
       " '##찬',\n",
       " '##했다',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens\n",
    "##  토큰 : 연결 subword(토큰) 구분자.\n",
    "# ## 이 없는 토큰은 원래 문자열로 되돌릴 때 앞에 공백을 붙인다.\n",
    "# ## 이 있는 토큰은 그대로 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7394,\n",
       " 8322,\n",
       " 8523,\n",
       " 8281,\n",
       " 8322,\n",
       " 8302,\n",
       " 6184,\n",
       " 8230,\n",
       " 15091,\n",
       " 6895,\n",
       " 8295,\n",
       " 8632,\n",
       " 16825,\n",
       " 8219,\n",
       " 16337,\n",
       " 14896,\n",
       " 14908,\n",
       " 5912,\n",
       " 8281,\n",
       " 8399,\n",
       " 8231,\n",
       " 5806,\n",
       " 8823,\n",
       " 8375,\n",
       " 8211,\n",
       " 4031,\n",
       " 15626,\n",
       " 15272,\n",
       " 6453,\n",
       " 8259,\n",
       " 4133,\n",
       " 9098,\n",
       " 15376,\n",
       " 19]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'프 ##리 ##미 ##어 ##리 ##그 역 ##대 개인 최 ##다 ##골 기록 ##을 보유 ##하고 있는 시 ##어 ##러 ##가 손 ##흥 ##민 ##의 골 결정 ##력을 재 ##차 극 ##찬 ##했다 .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_str = []\n",
    "for token in encoding.tokens:\n",
    "    if token.startswith(token[2:]):\n",
    "        decode_str.append(token[2:])\n",
    "    else : \n",
    "        decode_str.append(''+token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Unigram 모델 학습\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "# 토크나이저 객체 생성\n",
    "uni_tokenizer = Tokenizer(\n",
    "    Unigram()\n",
    ")\n",
    "\n",
    "# Pre tokenizer 생성\n",
    "uni_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# trainer 생성\n",
    "trainer = UnigramTrainer(\n",
    "    vocab_size=10000,\n",
    "    special_tokens=['<unk>','pad'],\n",
    "    min_frequeny=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 걸린 시간 : 489.81657099723816 초\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "uni_tokenizer.train(['data/petitions/petition_corpus.txt'],trainer=trainer)\n",
    "\n",
    "print('학습에 걸린 시간 :',time.time()-s, '초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tokenizer.save('saved_models/petitions_unigram.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_uni_tokenizer = Tokenizer.from_file('saved_models/petitions_unigram.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=47, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = uni_tokenizer.encode(comment_txt)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['멋',\n",
       " '진',\n",
       " '식',\n",
       " '사',\n",
       " '를',\n",
       " '즐',\n",
       " '기',\n",
       " '기에',\n",
       " '좋은',\n",
       " '장',\n",
       " '소',\n",
       " '-',\n",
       " '채',\n",
       " '식',\n",
       " '메',\n",
       " '뉴',\n",
       " '가',\n",
       " '정말',\n",
       " '훌',\n",
       " '륭',\n",
       " '했습니다',\n",
       " '.',\n",
       " '당',\n",
       " '근',\n",
       " '케',\n",
       " '이',\n",
       " '크',\n",
       " '는',\n",
       " '아',\n",
       " '마',\n",
       " '도',\n",
       " '내가',\n",
       " '먹',\n",
       " '어',\n",
       " '본',\n",
       " '디',\n",
       " '저',\n",
       " '트',\n",
       " '중',\n",
       " '최',\n",
       " '고',\n",
       " '였',\n",
       " '을',\n",
       " '거',\n",
       " '예',\n",
       " '요',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2663,\n",
       " 89,\n",
       " 162,\n",
       " 36,\n",
       " 10,\n",
       " 2231,\n",
       " 25,\n",
       " 710,\n",
       " 482,\n",
       " 60,\n",
       " 67,\n",
       " 114,\n",
       " 354,\n",
       " 162,\n",
       " 789,\n",
       " 2401,\n",
       " 8,\n",
       " 250,\n",
       " 2675,\n",
       " 2756,\n",
       " 282,\n",
       " 2,\n",
       " 85,\n",
       " 471,\n",
       " 1553,\n",
       " 3,\n",
       " 509,\n",
       " 11,\n",
       " 63,\n",
       " 127,\n",
       " 9,\n",
       " 1192,\n",
       " 435,\n",
       " 41,\n",
       " 311,\n",
       " 740,\n",
       " 123,\n",
       " 300,\n",
       " 68,\n",
       " 739,\n",
       " 13,\n",
       " 978,\n",
       " 4,\n",
       " 71,\n",
       " 298,\n",
       " 69,\n",
       " 2]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
