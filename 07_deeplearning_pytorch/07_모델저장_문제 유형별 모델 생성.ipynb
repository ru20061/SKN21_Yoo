{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 저장\n",
    "\n",
    "- 학습이 완료된 모델을 파일로 저장하여, 이후 추가 학습이나 예측 서비스에 사용할 수 있도록 한다.\n",
    "- 파이토치(PyTorch)는 **모델의 파라미터만 저장**하는 방법과 **모델의 구조와 파라미터를 모두 저장**하는 두 가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "  - `torch.save(저장할 객체, 저장 경로)`\n",
    "- 보통 저장 파일의 확장자는 `.pt`나 `.pth`를 사용한다.\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "  - `torch.save(model, 저장 경로)`\n",
    "- 불러오기\n",
    "  - `load_model = torch.load(저장 경로)`\n",
    "- 모델 저장 시 **피클(pickle)**을 사용해 직렬화되므로, 모델을 불러오는 실행 환경에도 저장할 때 사용한 클래스 정의가 필요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 저장 및 불러오기\n",
    "\n",
    "- 학습이 끝나지 않은 모델을 저장하고, 나중에 이어서 학습시키려면 모델의 구조와 파라미터뿐만 아니라 optimizer, loss 함수 등 학습에 필요한 객체들도 함께 저장해야 한다.\n",
    "- 딕셔너리(Dictionary)에 저장하려는 값들을 key-value 쌍으로 구성하여 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 불러온 checkpoint를 이용해 이전 학습 상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 모델 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3, 4) # 3 X 4 + 4 \n",
    "        self.lr2 = nn.Linear(4, 2)\n",
    "        self.relu = nn.ReLU() # activation함수->파라미터가 없는 단순 계산함수. relu(X) = max(X, 0)\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성성\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "#  모델에 Layer들을 조회. 모델.instance변수명\n",
    "################################################\n",
    "lr_layer = model.lr1\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#  Layer의 파라미터(weight/bias) 조회\n",
    "################################################\n",
    "lr1_weight = lr_layer.weight\n",
    "lr1_bias = lr_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0355,  0.3490, -0.1643],\n",
       "        [-0.2801, -0.2739, -0.5457],\n",
       "        [-0.4415, -0.0297,  0.3786],\n",
       "        [ 0.0406,  0.4998, -0.1034]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3467, -0.0845, -0.4996, -0.0269], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#  모델을 저장\n",
    "################################################\n",
    "torch.save(model, \"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "#  저장된 모델 Load\n",
    "################################################\n",
    "load_model = torch.load(\"saved_models/my_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0355,  0.3490, -0.1643],\n",
       "        [-0.2801, -0.2739, -0.5457],\n",
       "        [-0.4415, -0.0297,  0.3786],\n",
       "        [ 0.0406,  0.4998, -0.1034]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0355,  0.3490, -0.1643],\n",
       "        [-0.2801, -0.2739, -0.5457],\n",
       "        [-0.4415, -0.0297,  0.3786],\n",
       "        [ 0.0406,  0.4998, -0.1034]], requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.0355,  0.3490, -0.1643],\n",
       "                      [-0.2801, -0.2739, -0.5457],\n",
       "                      [-0.4415, -0.0297,  0.3786],\n",
       "                      [ 0.0406,  0.4998, -0.1034]])),\n",
       "             ('lr1.bias', tensor([ 0.3467, -0.0845, -0.4996, -0.0269])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.3335, -0.3852, -0.3446,  0.2252],\n",
       "                      [ 0.3588,  0.4776, -0.2790,  0.1461]])),\n",
       "             ('lr2.bias', tensor([-0.4323, -0.2951]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# 모델의 파라미터들(weight들, bias들)만 저장/불러오기\n",
    "######################################################\n",
    "state_dict = model.state_dict()\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# state_dict 저장\n",
    "################### \n",
    "\n",
    "torch.save(state_dict, \"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# state_dict load\n",
    "#####################\n",
    "sd = torch.load(\"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.4270,  0.0882, -0.5171],\n",
       "                      [ 0.1044, -0.5147, -0.2236],\n",
       "                      [ 0.1398, -0.1062,  0.3185],\n",
       "                      [-0.2263, -0.5404, -0.1163]])),\n",
       "             ('lr1.bias', tensor([ 0.4479, -0.4328,  0.4422, -0.3472])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.2066, -0.4670,  0.4441, -0.4921],\n",
       "                      [ 0.0881, -0.4068, -0.2147,  0.4300]])),\n",
       "             ('lr2.bias', tensor([ 0.2005, -0.0150]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load한 state_dict를 모델 파라미터에 적용(덮어 씌운다.)\n",
    "new_model = MyModel()\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.0355,  0.3490, -0.1643],\n",
       "                      [-0.2801, -0.2739, -0.5457],\n",
       "                      [-0.4415, -0.0297,  0.3786],\n",
       "                      [ 0.0406,  0.4998, -0.1034]])),\n",
       "             ('lr1.bias', tensor([ 0.3467, -0.0845, -0.4996, -0.0269])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.3335, -0.3852, -0.3446,  0.2252],\n",
       "                      [ 0.3588,  0.4776, -0.2790,  0.1461]])),\n",
       "             ('lr2.bias', tensor([-0.4323, -0.2951]))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 71ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 34ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchinfo\u001b[0m\u001b[2m==1.8.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# torchinfo 패키지 설치: 파이토치 모델 구조를 조사해주는 패키지.\n",
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data 의 shape을 지정하면 각 Layer의 output shape을 출력한다.\n",
    "summary(model, (100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다. \n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Boston Housing Dataset - **Regression(회귀) 문제**\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/boston_housing.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns='MEDV').values\n",
    "y = df['MEDV'].values\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 102)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train,dtype=torch.float32),\n",
    "    torch.tensor(y_train,dtype=torch.float32)\n",
    ")\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test,dtype=torch.float32),\n",
    "    torch.tensor(y_test,dtype=torch.float32)\n",
    ")\n",
    "\n",
    "len(trainset) , len(testset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 200\n",
    "train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(testset,batch_size=len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 모델 정의\n",
    "#################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class BostonModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(13,32) # 첫번째 연산\n",
    "        self.lr2 = nn.Linear(32,16)\n",
    "        self.lr3 = nn.Linear(16,1)    # 추론 결과를 출력\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,X):\n",
    "        # Linear - > ReLU -> Linear - > ReLU -> Linear -> 출력 \n",
    "        out = self.lr1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.lr2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available () else 'cpu'\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "# 모델 생성\n",
    "boston_model = BostonModel().to(device)\n",
    "# Loss 함수\n",
    "loss_fn = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(),lr =lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 448\n",
       "├─ReLU: 1-2                              [100, 32]                 --\n",
       "├─Linear: 1-3                            [100, 16]                 528\n",
       "├─ReLU: 1-4                              [100, 16]                 --\n",
       "├─Linear: 1-5                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(boston_model,(100,13),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\Documents\\SKN21_Yoo\\07_deeplearning_pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([200])) that is different to the input size (torch.Size([200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Playdata\\Documents\\SKN21_Yoo\\07_deeplearning_pytorch\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([102])) that is different to the input size (torch.Size([102, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/0] train_loss : 1190.78381, validation_loss : 515.63776\n",
      "[101/100] train_loss : 193.13811, validation_loss : 83.59399\n",
      "[201/200] train_loss : 181.27630, validation_loss : 78.34252\n",
      "[301/300] train_loss : 178.65058, validation_loss : 77.07653\n",
      "[401/400] train_loss : 176.42942, validation_loss : 76.56023\n",
      "[501/500] train_loss : 174.34029, validation_loss : 76.37154\n",
      "[601/600] train_loss : 176.33964, validation_loss : 76.18123\n",
      "[701/700] train_loss : 176.52665, validation_loss : 76.24580\n",
      "[801/800] train_loss : 174.81928, validation_loss : 76.16862\n",
      "[901/900] train_loss : 175.20389, validation_loss : 75.39146\n",
      "[1000/999] train_loss : 175.37435, validation_loss : 75.59916\n"
     ]
    }
   ],
   "source": [
    "# Train - > Train / Validation 두 단계\n",
    "for epoch in range(epochs):\n",
    "    ##########################\n",
    "    # 학습(train)\n",
    "    ##########################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0 # 현 epoch의 loss값을 저장할 변수\n",
    "    for X_train, y_train in train_loader: # 1 배치 학습\n",
    "        # 1. X,y 를 device 로 이동. (model 과 같은 device 로 이동)\n",
    "        X_train, y_train = X_train.to(device) , y_train.to(device)\n",
    "\n",
    "        # 2. 추론\n",
    "        pred_train = boston_model(X_train)\n",
    "\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred_train,y_train)\n",
    "\n",
    "        # 4.역전파로 gradient 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 6. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 7. 현재 batch에 대한 loss 계산\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "\n",
    "    ##########################\n",
    "    # 검증(validation)\n",
    "    ##########################\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = 0.0 # 현 epoch에 대한 검증 loss값을 저장할 변수\n",
    "    with torch.no_grad():\n",
    "        for X_valid,y_valid in test_loader:\n",
    "            # 1. X,y 를 model의 device 로 이동.\n",
    "            X_valid,y_valid = X_valid.to(device),y_valid.to(device)\n",
    "\n",
    "            # 2. 추론\n",
    "            pred_valid = boston_model(X_valid)\n",
    "\n",
    "            #3. 평가 - MSE\n",
    "            valid_loss = loss_fn(pred_valid,y_valid).item()\n",
    "        valid_loss /= len(test_loader)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "    # 로그 출력 train/valid loss를 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f'[{epoch+1}/{epoch}] train_loss : {train_loss:.5f}, validation_loss : {valid_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_boston_model_path = 'saved_models/boston_model.pth'\n",
    "torch.save(boston_model,save_boston_model_path)\n",
    "torch.save(boston_model.state_dict(),'saved_models/boston_model_parameter.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.2140],\n",
      "        [22.6164],\n",
      "        [23.5124],\n",
      "        [23.9293],\n",
      "        [22.8345],\n",
      "        [23.0422],\n",
      "        [22.1576],\n",
      "        [22.2386],\n",
      "        [23.2756],\n",
      "        [22.5179]])\n"
     ]
    }
   ],
   "source": [
    "load_model = torch.load(save_boston_model_path,weights_only=False)\n",
    "load_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(load_model(X_valid)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[23.2140],\n",
      "        [22.6164],\n",
      "        [23.5124],\n",
      "        [23.9293],\n",
      "        [22.8345],\n",
      "        [23.0422],\n",
      "        [22.1576],\n",
      "        [22.2386],\n",
      "        [23.2756],\n",
      "        [22.5179]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_state_dict = torch.load('saved_models/boston_model_parameter.pth',weights_only = False)\n",
    "new_model = BostonModel()\n",
    "new_model.load_state_dict(load_state_dict)\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    result = new_model(X_valid)[:10]\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - **다중분류(Multi-Class Classification) 문제**\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 학습 도중 모델 저장\n",
    ">\n",
    "> - 학습 도중 가장 좋은 성능을 보이는 모델이 나올 수 있다.\n",
    "> - 학습 도중 모델을 저장하는 방법\n",
    ">   1. 각 에폭이 끝날 때 마다 모델을 저장한다.\n",
    ">   2. 한 에폭 학습 후 성능 개선이 있으면 모델을 저장하여 가장 성능 좋은 모델만 저장되도록 한다.\n",
    ">      - 최고 성능 점수(best score)와 현재 에폭의 성능을 비교하여, 성능이 개선되었을 경우 모델을 저장(덮어쓰기)한다.\n",
    ">\n",
    "> #### 조기 종료(Early Stopping)\n",
    ">\n",
    "> - 학습 도중 성능 개선이 나타나지 않으면, 중간에 학습을 종료하도록 구현한다.\n",
    "> - 에폭 수를 충분히 길게 설정한 뒤, 특정 횟수 동안 성능 개선이 없으면 학습을 조기 종료하도록 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000, 195, 40)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset\n",
    "dataset_path = 'dataset/fashion_mnist'\n",
    "batch_size=256\n",
    "f_trainset = FashionMNIST(dataset_path,train=True,download=True,transform=transforms.ToTensor())\n",
    "f_testset = FashionMNIST(dataset_path,train=False,download=True,transform=transforms.ToTensor())\n",
    "f_trainset, f_validset = random_split(f_trainset,[50000,10000])\n",
    "\n",
    "f_train_loader = DataLoader(f_trainset,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "f_valid_loader = DataLoader(f_validset,batch_size=batch_size)\n",
    "f_test_loader = DataLoader(f_testset,batch_size=batch_size)\n",
    "\n",
    "len(f_trainset) , len(f_testset) ,len(f_validset) , len(f_train_loader) , len(f_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(f_trainset[0][0].dtype)\n",
    "print(f_trainset[0][0].size())\n",
    "print(f_trainset[0][0].min(),f_trainset[0][0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# 모델 정의\n",
    "#################\n",
    "\n",
    "class FashionMNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1=nn.Linear(784,2048)\n",
    "        self.lr2=nn.Linear(2048,1024)\n",
    "        self.lr3=nn.Linear(1024,512)\n",
    "        self.lr4=nn.Linear(512,256)\n",
    "        self.lr5=nn.Linear(256,128)\n",
    "        self.lr6=nn.Linear(128,64)\n",
    "        self.lr7=nn.Linear(64,10)\n",
    "        # 출력 Layer out_features 개수 : 다중분류문제의 경우 정답 클래스 개수.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "        # X shape: (batch,1,28,28) -> (batch,784) torch.flatten(start_dim=1)함수\n",
    "        out = nn.Flatten()(X) # 0축은 그대로 두고 그 이후 축을 flatten 한다.\n",
    "        out = self.lr1(out) # self.relu(self.lr1(out))\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr2(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.lr3(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.lr4(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.lr5(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.lr6(out) \n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.lr7(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [100, 10]                 --\n",
       "├─Linear: 1-1                            [100, 2048]               1,607,680\n",
       "├─ReLU: 1-2                              [100, 2048]               --\n",
       "├─Linear: 1-3                            [100, 1024]               2,098,176\n",
       "├─ReLU: 1-4                              [100, 1024]               --\n",
       "├─Linear: 1-5                            [100, 512]                524,800\n",
       "├─ReLU: 1-6                              [100, 512]                --\n",
       "├─Linear: 1-7                            [100, 256]                131,328\n",
       "├─ReLU: 1-8                              [100, 256]                --\n",
       "├─Linear: 1-9                            [100, 128]                32,896\n",
       "├─ReLU: 1-10                             [100, 128]                --\n",
       "├─Linear: 1-11                           [100, 64]                 8,256\n",
       "├─ReLU: 1-12                             [100, 64]                 --\n",
       "├─Linear: 1-13                           [100, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 440.38\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 3.23\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 21.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(FashionMNISTModel(),(100,1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = FashionMNISTModel().to(device)\n",
    "optimizer = torch.optim.Adam(f_model.parameters(),lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "# 다중분류의 loss : CrossEntropyLoss\n",
    "#  CrossEntropyLoss()입력 : 정답-class index 로 구성 [[3],[8],[0]...], 모델 추정결과 : Softmax를 적용하기 전값 = logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] train_loss : 1.15894, valid_loss : 0.73548, valid_acc : 0.73590\n",
      ">>>>>>> 모델 저장 : 1 epoch - 이전 score : inf , 개선된 score : 0.7354829505085945\n",
      "[2/100] train_loss : 0.62372, valid_loss : 0.58376, valid_acc : 0.78920\n",
      ">>>>>>> 모델 저장 : 2 epoch - 이전 score : 0.7354829505085945 , 개선된 score : 0.5837574489414692\n",
      "[3/100] train_loss : 0.53521, valid_loss : 0.56680, valid_acc : 0.79740\n",
      ">>>>>>> 모델 저장 : 3 epoch - 이전 score : 0.5837574489414692 , 개선된 score : 0.5668026894330979\n",
      "[4/100] train_loss : 0.49426, valid_loss : 0.48880, valid_acc : 0.82850\n",
      ">>>>>>> 모델 저장 : 4 epoch - 이전 score : 0.5668026894330979 , 개선된 score : 0.4888020746409893\n",
      "[5/100] train_loss : 0.46103, valid_loss : 0.45876, valid_acc : 0.83980\n",
      ">>>>>>> 모델 저장 : 5 epoch - 이전 score : 0.4888020746409893 , 개선된 score : 0.45876314416527747\n",
      "[6/100] train_loss : 0.42642, valid_loss : 0.43288, valid_acc : 0.84780\n",
      ">>>>>>> 모델 저장 : 6 epoch - 이전 score : 0.45876314416527747 , 개선된 score : 0.43287878334522245\n",
      "[7/100] train_loss : 0.40699, valid_loss : 0.45329, valid_acc : 0.84170\n",
      "[8/100] train_loss : 0.38707, valid_loss : 0.39990, valid_acc : 0.86070\n",
      ">>>>>>> 모델 저장 : 8 epoch - 이전 score : 0.43287878334522245 , 개선된 score : 0.39990306571125983\n",
      "[9/100] train_loss : 0.36980, valid_loss : 0.39939, valid_acc : 0.85980\n",
      ">>>>>>> 모델 저장 : 9 epoch - 이전 score : 0.39990306571125983 , 개선된 score : 0.39939432963728905\n",
      "[10/100] train_loss : 0.35610, valid_loss : 0.41006, valid_acc : 0.85270\n",
      "[11/100] train_loss : 0.33982, valid_loss : 0.37142, valid_acc : 0.87060\n",
      ">>>>>>> 모델 저장 : 11 epoch - 이전 score : 0.39939432963728905 , 개선된 score : 0.3714234627783298\n",
      "[12/100] train_loss : 0.33100, valid_loss : 0.36532, valid_acc : 0.87200\n",
      ">>>>>>> 모델 저장 : 12 epoch - 이전 score : 0.3714234627783298 , 개선된 score : 0.36532138362526895\n",
      "[13/100] train_loss : 0.31794, valid_loss : 0.36967, valid_acc : 0.87070\n",
      "[14/100] train_loss : 0.31332, valid_loss : 0.34053, valid_acc : 0.87490\n",
      ">>>>>>> 모델 저장 : 14 epoch - 이전 score : 0.36532138362526895 , 개선된 score : 0.34053046703338624\n",
      "[15/100] train_loss : 0.29774, valid_loss : 0.33657, valid_acc : 0.88070\n",
      ">>>>>>> 모델 저장 : 15 epoch - 이전 score : 0.34053046703338624 , 개선된 score : 0.33657065257430074\n",
      "[16/100] train_loss : 0.28848, valid_loss : 0.34599, valid_acc : 0.87650\n",
      "[17/100] train_loss : 0.27807, valid_loss : 0.33389, valid_acc : 0.87980\n",
      ">>>>>>> 모델 저장 : 17 epoch - 이전 score : 0.33657065257430074 , 개선된 score : 0.33388707265257833\n",
      "[18/100] train_loss : 0.27073, valid_loss : 0.34213, valid_acc : 0.88100\n",
      "[19/100] train_loss : 0.25998, valid_loss : 0.33518, valid_acc : 0.88200\n",
      "[20/100] train_loss : 0.25415, valid_loss : 0.33529, valid_acc : 0.87910\n",
      "[21/100] train_loss : 0.24884, valid_loss : 0.31832, valid_acc : 0.88640\n",
      ">>>>>>> 모델 저장 : 21 epoch - 이전 score : 0.33388707265257833 , 개선된 score : 0.3183217212557793\n",
      "[22/100] train_loss : 0.24164, valid_loss : 0.33062, valid_acc : 0.88380\n",
      "[23/100] train_loss : 0.23184, valid_loss : 0.31731, valid_acc : 0.88650\n",
      ">>>>>>> 모델 저장 : 23 epoch - 이전 score : 0.3183217212557793 , 개선된 score : 0.31730768643319607\n",
      "[24/100] train_loss : 0.22432, valid_loss : 0.32112, valid_acc : 0.88520\n",
      "[25/100] train_loss : 0.22548, valid_loss : 0.33638, valid_acc : 0.88390\n",
      "[26/100] train_loss : 0.21804, valid_loss : 0.31689, valid_acc : 0.88720\n",
      ">>>>>>> 모델 저장 : 26 epoch - 이전 score : 0.31730768643319607 , 개선된 score : 0.3168866477906704\n",
      "[27/100] train_loss : 0.20849, valid_loss : 0.32880, valid_acc : 0.88510\n",
      "[28/100] train_loss : 0.20433, valid_loss : 0.33312, valid_acc : 0.88570\n",
      "[29/100] train_loss : 0.19649, valid_loss : 0.31387, valid_acc : 0.89130\n",
      ">>>>>>> 모델 저장 : 29 epoch - 이전 score : 0.3168866477906704 , 개선된 score : 0.3138706274330616\n",
      "[30/100] train_loss : 0.18844, valid_loss : 0.34687, valid_acc : 0.88430\n",
      "[31/100] train_loss : 0.18545, valid_loss : 0.31309, valid_acc : 0.89440\n",
      ">>>>>>> 모델 저장 : 31 epoch - 이전 score : 0.3138706274330616 , 개선된 score : 0.31309070661664007\n",
      "[32/100] train_loss : 0.17907, valid_loss : 0.34121, valid_acc : 0.89130\n",
      "[33/100] train_loss : 0.17271, valid_loss : 0.33576, valid_acc : 0.89290\n",
      "[34/100] train_loss : 0.16914, valid_loss : 0.32861, valid_acc : 0.89400\n",
      "[35/100] train_loss : 0.16024, valid_loss : 0.33719, valid_acc : 0.89410\n",
      "[36/100] train_loss : 0.15022, valid_loss : 0.32846, valid_acc : 0.89800\n",
      "[37/100] train_loss : 0.15059, valid_loss : 0.33336, valid_acc : 0.89680\n",
      "[38/100] train_loss : 0.14666, valid_loss : 0.35708, valid_acc : 0.89070\n",
      "[39/100] train_loss : 0.13803, valid_loss : 0.34602, valid_acc : 0.89270\n",
      "[40/100] train_loss : 0.13691, valid_loss : 0.34838, valid_acc : 0.89490\n",
      "[41/100] train_loss : 0.13021, valid_loss : 0.37139, valid_acc : 0.88470\n",
      ">>>>>>>>>>>> 조기종료 : 0.31309070661664007 보다 개선되지 않음.\n",
      "학습에 걸린 시간 :  434.1117515563965  초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "######################\n",
    "# 1. 성능이 개선될 때 마다 모델을 저장.\n",
    "# 최종적으로 가장 성능이 좋은 epoch의 모델이 저장되도록\n",
    "# 2. 일정 epoch 동산 성능개선이 안되면 학습을 중단.\n",
    "# 이것과 관련된 변수들 정의\n",
    "######################\n",
    "\n",
    "best_score = torch.inf #가장 좋은 valid_loss 값을 저장\n",
    "# 저장\n",
    "save_model_path = 'saved_models/fashion_mnist_model.pth'\n",
    "# 조기종료\n",
    "patience = 10 # 성능이 개선되는지 10 에폭 동안 기다려본다.\n",
    "stop_count = 0 # 학습하는 도중 몇 번동안 성능개선이 안되었는지 저장할변수. stop_count == patience 이면 종료.\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # 학습 \n",
    "    f_model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in f_train_loader:\n",
    "        # 1. X,y를 device 로 이동\n",
    "        X_train, y_train = X_train.to(device) , y_train.to(device)\n",
    "        # 2. 모델 추론\n",
    "        pred_train = f_model(X_train)\n",
    "        # 3. loss 계산\n",
    "        loss = loss_fn(pred_train,y_train)\n",
    "        # 4. gradient 계산 (역전파)\n",
    "        loss.backward()\n",
    "        # 5. 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 6. 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 7. 현제 에폭에서의 train loss 누적\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    #train loss 평균계산\n",
    "    train_loss /=len(f_train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    # 검증\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_valid,y_valid in f_valid_loader:\n",
    "            # 1. device 이동\n",
    "            X_valid,y_valid = X_valid.to(device),y_valid.to(device)\n",
    "\n",
    "            # 2. 모델 추정\n",
    "            pred_valid = f_model(X_valid) # 10개 class 에 대한 logit\n",
    "            pred_label = pred_valid.argmax(dim=1) # 가장 확률높은 class\n",
    "\n",
    "            valid_loss += loss_fn(pred_valid,y_valid).item() #현 스텝에서의 loss\n",
    "            valid_acc += torch.sum(pred_label == y_valid).item() #현 스텝에서 맞은 것 개수.\n",
    "\n",
    "        valid_loss /= len(f_valid_loader)\n",
    "        valid_acc /= len(f_validset)\n",
    "    \n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        \n",
    "        #로그 출력 - train_loss , valid_loss , valid_acc\n",
    "        print(f'[{epoch+1}/{epochs}] train_loss : {train_loss:.5f}, valid_loss : {valid_loss:.5f}, valid_acc : {valid_acc:.5f}')\n",
    "\n",
    "        #########################\n",
    "        # 모델 저장 , 조기 종료\n",
    "        #########################\n",
    "\n",
    "        if valid_loss < best_score: # 이번 epoch 학습엣 성능(valid_loss)이 개선되었는지\n",
    "            # 모델 저장\n",
    "            torch.save(f_model,save_model_path)\n",
    "            print(f'>>>>>>> 모델 저장 : {epoch+1} epoch - 이전 score : {best_score} , 개선된 score : {valid_loss}')\n",
    "            best_score = valid_loss\n",
    "            # 조기 종료 (stop_count를 초기화)\n",
    "            stop_count = 0\n",
    "        else: # 성능 개선이 안됨. -> 조기 종료\n",
    "            stop_count +=1\n",
    "            if stop_count == patience : \n",
    "                print (f'>>>>>>>>>>>> 조기종료 : {best_score} 보다 개선되지 않음.')\n",
    "                break\n",
    "            \n",
    "\n",
    "e = time.time()\n",
    "\n",
    "\n",
    "print('학습에 걸린 시간 : ',(e-s),' 초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "result_dict = {\n",
    "    'train_loss': train_loss_list,\n",
    "    'valid_loss': valid_loss_list,\n",
    "    'valid_acc': valid_acc_list\n",
    "}\n",
    "\n",
    "with open('f_mnist_train_result.pkl','wb') as fo:\n",
    "    pickle.dump(result_dict, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - **이진분류(Binary Classification) 문제**\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "## 모델의 weight, bias -> float32. X, y는 weight, bias와 계산을 하게 되기 때문에 타입을 맞춰준다.\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),  \n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name <-> class index\n",
    "classes = np.array([\"악성종양\", \"양성종양\"])\n",
    "class_to_idx = {\"악성종양\":0, \"양성종양\":1}\n",
    "\n",
    "trainset.classes = classes\n",
    "trainset.class_to_idx = class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 모델 정의\n",
    "class BCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1=nn.Linear(30,16)\n",
    "        self.lr2=nn.Linear(16,8)\n",
    "        self.lr3=nn.Linear(8,1) # out_features : 1 - positive일 확률\n",
    "        self.relu = nn.ReLU() # hidden layer의 활성함수\n",
    "        self.sigmoid = nn.Sigmoid() # Linear 출력값을 0 ~ 1 확률로 만들어 주는 Sigmoid(Logistic) 함수.\n",
    "\n",
    "    def forward(self,X):\n",
    "        out= self.relu(self.lr1(X))\n",
    "        out= self.relu(self.lr2(out))\n",
    "        \n",
    "        out = self.lr3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5971],\n",
       "        [0.5899],\n",
       "        [0.5825],\n",
       "        [0.5649],\n",
       "        [0.5819],\n",
       "        [0.5731],\n",
       "        [0.5618],\n",
       "        [0.5826],\n",
       "        [0.5648],\n",
       "        [0.5586]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BCModel()\n",
    "p = m(torch.randn(10,30))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 학습\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model = BCModel().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "#조기종료 , best 모델 지정\n",
    "best_score = torch.inf  # valid loss 기준\n",
    "save_model_path = \"saved_models/bc_model.pth\"\n",
    "patience = 10\n",
    "stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train_loss : 0.69035, valid_loss : 0.66595, valid_acc : 0.86713\n",
      ">>>>>>>>>> 1모델을 저장합니다. inf 에서 0.6659479737281799 로 개선됨\n",
      "[2/1000] train_loss : 0.65690, valid_loss : 0.63202, valid_acc : 0.88811\n",
      ">>>>>>>>>> 2모델을 저장합니다. 0.6659479737281799 에서 0.6320174634456635 로 개선됨\n",
      "[3/1000] train_loss : 0.61326, valid_loss : 0.59132, valid_acc : 0.90210\n",
      ">>>>>>>>>> 3모델을 저장합니다. 0.6320174634456635 에서 0.5913179814815521 로 개선됨\n",
      "[4/1000] train_loss : 0.56640, valid_loss : 0.54829, valid_acc : 0.90210\n",
      ">>>>>>>>>> 4모델을 저장합니다. 0.5913179814815521 에서 0.5482855141162872 로 개선됨\n",
      "[5/1000] train_loss : 0.52146, valid_loss : 0.50936, valid_acc : 0.90210\n",
      ">>>>>>>>>> 5모델을 저장합니다. 0.5482855141162872 에서 0.5093619227409363 로 개선됨\n",
      "[6/1000] train_loss : 0.48355, valid_loss : 0.47845, valid_acc : 0.93007\n",
      ">>>>>>>>>> 6모델을 저장합니다. 0.5093619227409363 에서 0.4784467965364456 로 개선됨\n",
      "[7/1000] train_loss : 0.45267, valid_loss : 0.45644, valid_acc : 0.92308\n",
      ">>>>>>>>>> 7모델을 저장합니다. 0.4784467965364456 에서 0.45643986761569977 로 개선됨\n",
      "[8/1000] train_loss : 0.42736, valid_loss : 0.44274, valid_acc : 0.91608\n",
      ">>>>>>>>>> 8모델을 저장합니다. 0.45643986761569977 에서 0.44273678958415985 로 개선됨\n",
      "[9/1000] train_loss : 0.40979, valid_loss : 0.43185, valid_acc : 0.93007\n",
      ">>>>>>>>>> 9모델을 저장합니다. 0.44273678958415985 에서 0.4318544417619705 로 개선됨\n",
      "[10/1000] train_loss : 0.39950, valid_loss : 0.42322, valid_acc : 0.94406\n",
      ">>>>>>>>>> 10모델을 저장합니다. 0.4318544417619705 에서 0.4232186824083328 로 개선됨\n",
      "[11/1000] train_loss : 0.38328, valid_loss : 0.41513, valid_acc : 0.94406\n",
      ">>>>>>>>>> 11모델을 저장합니다. 0.4232186824083328 에서 0.4151250869035721 로 개선됨\n",
      "[12/1000] train_loss : 0.37217, valid_loss : 0.40599, valid_acc : 0.95105\n",
      ">>>>>>>>>> 12모델을 저장합니다. 0.4151250869035721 에서 0.4059906452894211 로 개선됨\n",
      "[13/1000] train_loss : 0.36786, valid_loss : 0.39624, valid_acc : 0.95804\n",
      ">>>>>>>>>> 13모델을 저장합니다. 0.4059906452894211 에서 0.3962376117706299 로 개선됨\n",
      "[14/1000] train_loss : 0.35178, valid_loss : 0.38257, valid_acc : 0.95804\n",
      ">>>>>>>>>> 14모델을 저장합니다. 0.3962376117706299 에서 0.3825707882642746 로 개선됨\n",
      "[15/1000] train_loss : 0.33031, valid_loss : 0.36182, valid_acc : 0.95804\n",
      ">>>>>>>>>> 15모델을 저장합니다. 0.3825707882642746 에서 0.36182451248168945 로 개선됨\n",
      "[16/1000] train_loss : 0.30836, valid_loss : 0.33381, valid_acc : 0.95105\n",
      ">>>>>>>>>> 16모델을 저장합니다. 0.36182451248168945 에서 0.3338118642568588 로 개선됨\n",
      "[17/1000] train_loss : 0.27885, valid_loss : 0.30128, valid_acc : 0.95105\n",
      ">>>>>>>>>> 17모델을 저장합니다. 0.3338118642568588 에서 0.30127672851085663 로 개선됨\n",
      "[18/1000] train_loss : 0.24380, valid_loss : 0.26492, valid_acc : 0.95105\n",
      ">>>>>>>>>> 18모델을 저장합니다. 0.30127672851085663 에서 0.2649175450205803 로 개선됨\n",
      "[19/1000] train_loss : 0.20918, valid_loss : 0.22722, valid_acc : 0.96503\n",
      ">>>>>>>>>> 19모델을 저장합니다. 0.2649175450205803 에서 0.22722448408603668 로 개선됨\n",
      "[20/1000] train_loss : 0.17098, valid_loss : 0.18989, valid_acc : 0.96503\n",
      ">>>>>>>>>> 20모델을 저장합니다. 0.22722448408603668 에서 0.1898869052529335 로 개선됨\n",
      "[21/1000] train_loss : 0.13494, valid_loss : 0.15589, valid_acc : 0.96503\n",
      ">>>>>>>>>> 21모델을 저장합니다. 0.1898869052529335 에서 0.15588626265525818 로 개선됨\n",
      "[22/1000] train_loss : 0.10221, valid_loss : 0.12840, valid_acc : 0.96503\n",
      ">>>>>>>>>> 22모델을 저장합니다. 0.15588626265525818 에서 0.12839829176664352 로 개선됨\n",
      "[23/1000] train_loss : 0.08124, valid_loss : 0.10716, valid_acc : 0.96503\n",
      ">>>>>>>>>> 23모델을 저장합니다. 0.12839829176664352 에서 0.10716155916452408 로 개선됨\n",
      "[24/1000] train_loss : 0.06424, valid_loss : 0.09344, valid_acc : 0.97203\n",
      ">>>>>>>>>> 24모델을 저장합니다. 0.10716155916452408 에서 0.09343802556395531 로 개선됨\n",
      "[25/1000] train_loss : 0.04745, valid_loss : 0.08523, valid_acc : 0.97203\n",
      ">>>>>>>>>> 25모델을 저장합니다. 0.09343802556395531 에서 0.08522781729698181 로 개선됨\n",
      "[26/1000] train_loss : 0.04747, valid_loss : 0.08066, valid_acc : 0.97203\n",
      ">>>>>>>>>> 26모델을 저장합니다. 0.08522781729698181 에서 0.0806636605411768 로 개선됨\n",
      "[27/1000] train_loss : 0.04321, valid_loss : 0.07788, valid_acc : 0.97203\n",
      ">>>>>>>>>> 27모델을 저장합니다. 0.0806636605411768 에서 0.07787718251347542 로 개선됨\n",
      "[28/1000] train_loss : 0.03037, valid_loss : 0.07624, valid_acc : 0.97203\n",
      ">>>>>>>>>> 28모델을 저장합니다. 0.07787718251347542 에서 0.07624074257910252 로 개선됨\n",
      "[29/1000] train_loss : 0.03904, valid_loss : 0.07515, valid_acc : 0.97203\n",
      ">>>>>>>>>> 29모델을 저장합니다. 0.07624074257910252 에서 0.07515253312885761 로 개선됨\n",
      "[30/1000] train_loss : 0.03734, valid_loss : 0.07431, valid_acc : 0.97902\n",
      ">>>>>>>>>> 30모델을 저장합니다. 0.07515253312885761 에서 0.07431387901306152 로 개선됨\n",
      "[31/1000] train_loss : 0.03767, valid_loss : 0.07367, valid_acc : 0.97902\n",
      ">>>>>>>>>> 31모델을 저장합니다. 0.07431387901306152 에서 0.07367011345922947 로 개선됨\n",
      "[32/1000] train_loss : 0.02986, valid_loss : 0.07331, valid_acc : 0.97902\n",
      ">>>>>>>>>> 32모델을 저장합니다. 0.07367011345922947 에서 0.07330916076898575 로 개선됨\n",
      "[33/1000] train_loss : 0.03637, valid_loss : 0.07231, valid_acc : 0.97902\n",
      ">>>>>>>>>> 33모델을 저장합니다. 0.07330916076898575 에서 0.07230573333799839 로 개선됨\n",
      "[34/1000] train_loss : 0.03460, valid_loss : 0.07141, valid_acc : 0.97902\n",
      ">>>>>>>>>> 34모델을 저장합니다. 0.07230573333799839 에서 0.07140838168561459 로 개선됨\n",
      "[35/1000] train_loss : 0.03490, valid_loss : 0.07069, valid_acc : 0.97902\n",
      ">>>>>>>>>> 35모델을 저장합니다. 0.07140838168561459 에서 0.07069489732384682 로 개선됨\n",
      "[36/1000] train_loss : 0.01996, valid_loss : 0.07076, valid_acc : 0.97902\n",
      "[37/1000] train_loss : 0.03353, valid_loss : 0.07154, valid_acc : 0.97203\n",
      "[38/1000] train_loss : 0.03314, valid_loss : 0.07298, valid_acc : 0.96503\n",
      "[39/1000] train_loss : 0.03187, valid_loss : 0.07498, valid_acc : 0.96503\n",
      "[40/1000] train_loss : 0.03194, valid_loss : 0.07735, valid_acc : 0.96503\n",
      "[41/1000] train_loss : 0.03063, valid_loss : 0.08004, valid_acc : 0.96503\n",
      "[42/1000] train_loss : 0.03005, valid_loss : 0.08364, valid_acc : 0.96503\n",
      "[43/1000] train_loss : 0.02838, valid_loss : 0.08887, valid_acc : 0.96503\n",
      "[44/1000] train_loss : 0.02855, valid_loss : 0.09600, valid_acc : 0.95804\n",
      "[45/1000] train_loss : 0.02722, valid_loss : 0.10210, valid_acc : 0.95804\n",
      ">>>>>>>>>> 45에서 조기종료 합니다. 0.07069489732384682 에서 개선이 안됨\n"
     ]
    }
   ],
   "source": [
    "######### 저장된 모델 로드\n",
    "for epoch in range(epochs):\n",
    "    # 학습\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_train,y_train in train_loader:\n",
    "        # 1. X,y를 device 로 이동\n",
    "        X_train, y_train = X_train.to(device) , y_train.to(device)\n",
    "        # 2. 모델 추론\n",
    "        pred_train = model(X_train)\n",
    "        # 3. loss 계산\n",
    "        loss = loss_fn(pred_train,y_train)\n",
    "        # 4. gradient 계산 (역전파)\n",
    "        loss.backward()\n",
    "        # 5. 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 6. 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 7. loss 누적\n",
    "        train_loss += loss.item()\n",
    "    #train loss 평균\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for X_valid,y_valid in test_loader:\n",
    "            # 1. device 이동\n",
    "            X_valid,y_valid = X_valid.to(device),y_valid.to(device)\n",
    "\n",
    "            # 2. 추론\n",
    "            pred_valid = model(X_valid) # (batch,1) 1축 : positve일 확률\n",
    "            pred_label = (pred_valid>0.5).type(torch.int32) # label\n",
    "\n",
    "            # 평가\n",
    "            valid_loss += loss_fn(pred_valid,y_valid).item() # loss\n",
    "            valid_acc += torch.sum(pred_label == y_valid).item() # acc\n",
    "\n",
    "        valid_loss /= len(test_loader)\n",
    "        valid_acc /= len(testset)\n",
    "    \n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f'[{epoch+1}/{epochs}] train_loss : {train_loss:.5f}, valid_loss : {valid_loss:.5f}, valid_acc : {valid_acc:.5f}')\n",
    "\n",
    "        # 성능 개선시 모델 저장  + 조기종료\n",
    "        if valid_loss < best_score : #개선됨\n",
    "            #모델 저장 + stop_count 초기화\n",
    "            print(f'>>>>>>>>>> {epoch+1}모델을 저장합니다. {best_score} 에서 {valid_loss} 로 개선됨')\n",
    "            torch.save(model, save_model_path)\n",
    "            best_score = valid_loss\n",
    "            stop_count = 0\n",
    "        else: # 개선 안됨\n",
    "            stop_count += 1\n",
    "            if patience == stop_count:\n",
    "                print(f'>>>>>>>>>> {epoch+1}에서 조기종료 합니다. {best_score} 에서 개선이 안됨')\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCModel(\n",
       "  (lr1): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (lr2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (lr3): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#저장된 모델 로드\n",
    "load_bc_model = torch.load(save_model_path, weights_only=False)\n",
    "load_bc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bool type label ex\n",
    "p = m(torch.randn(10,30))\n",
    "(p>0.5).type(torch.int32) # bool -> int : True -1 ,False -0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 추론 함수 #######\n",
    "def predict_bc(model, X, device=\"cpu\"):\n",
    "    # model로 X를 추론한 결과를 반환\n",
    "    # label, 확률\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        pred_proba = model(X)\n",
    "        pred_class = (pred_proba > 0.5).type(torch.int32)\n",
    "        for class_index, proba in zip(pred_class, pred_proba):\n",
    "            # print(class_index, proba if class_index.item() == 1 else 1-proba)\n",
    "            result.append((class_index.item(), proba.item() if class_index.item() == 1 else (1-proba.item())))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9975098371505737),\n",
       " (0, 0.9999999956312995),\n",
       " (0, 0.9999993686446942),\n",
       " (1, 0.9999111890792847),\n",
       " (0, 0.9999732061987743)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = predict_bc(load_bc_model,new_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BCModel().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/1000] - Train loss: 0.67202 Train Accucracy: 0.59624 || Validation Loss: 0.66895 Validation Accuracy: 0.64336\n",
      "====================================================================================================\n",
      "저장: 1 - 이전 : inf, 현재: 0.6689500212669373\n",
      "Epoch[2/1000] - Train loss: 0.66406 Train Accucracy: 0.63146 || Validation Loss: 0.66225 Validation Accuracy: 0.66434\n",
      "====================================================================================================\n",
      "저장: 2 - 이전 : 0.6689500212669373, 현재: 0.6622499525547028\n",
      "Epoch[3/1000] - Train loss: 0.65799 Train Accucracy: 0.65728 || Validation Loss: 0.65555 Validation Accuracy: 0.69231\n",
      "====================================================================================================\n",
      "저장: 3 - 이전 : 0.6622499525547028, 현재: 0.6555511951446533\n",
      "Epoch[4/1000] - Train loss: 0.64912 Train Accucracy: 0.70188 || Validation Loss: 0.64892 Validation Accuracy: 0.72028\n",
      "====================================================================================================\n",
      "저장: 4 - 이전 : 0.6555511951446533, 현재: 0.6489169895648956\n",
      "Epoch[5/1000] - Train loss: 0.64296 Train Accucracy: 0.72066 || Validation Loss: 0.64231 Validation Accuracy: 0.74825\n",
      "====================================================================================================\n",
      "저장: 5 - 이전 : 0.6489169895648956, 현재: 0.6423135697841644\n",
      "Epoch[6/1000] - Train loss: 0.63701 Train Accucracy: 0.75117 || Validation Loss: 0.63582 Validation Accuracy: 0.78322\n",
      "====================================================================================================\n",
      "저장: 6 - 이전 : 0.6423135697841644, 현재: 0.6358166635036469\n",
      "Epoch[7/1000] - Train loss: 0.62860 Train Accucracy: 0.80282 || Validation Loss: 0.62925 Validation Accuracy: 0.81119\n",
      "====================================================================================================\n",
      "저장: 7 - 이전 : 0.6358166635036469, 현재: 0.6292451024055481\n",
      "Epoch[8/1000] - Train loss: 0.62201 Train Accucracy: 0.81221 || Validation Loss: 0.62264 Validation Accuracy: 0.83217\n",
      "====================================================================================================\n",
      "저장: 8 - 이전 : 0.6292451024055481, 현재: 0.6226363480091095\n",
      "Epoch[9/1000] - Train loss: 0.61674 Train Accucracy: 0.81221 || Validation Loss: 0.61602 Validation Accuracy: 0.85315\n",
      "====================================================================================================\n",
      "저장: 9 - 이전 : 0.6226363480091095, 현재: 0.6160247027873993\n",
      "Epoch[10/1000] - Train loss: 0.60901 Train Accucracy: 0.81221 || Validation Loss: 0.60952 Validation Accuracy: 0.84615\n",
      "====================================================================================================\n",
      "저장: 10 - 이전 : 0.6160247027873993, 현재: 0.6095168590545654\n",
      "Epoch[11/1000] - Train loss: 0.60113 Train Accucracy: 0.82864 || Validation Loss: 0.60287 Validation Accuracy: 0.86014\n",
      "====================================================================================================\n",
      "저장: 11 - 이전 : 0.6095168590545654, 현재: 0.6028681993484497\n",
      "Epoch[12/1000] - Train loss: 0.59487 Train Accucracy: 0.83333 || Validation Loss: 0.59605 Validation Accuracy: 0.87413\n",
      "====================================================================================================\n",
      "저장: 12 - 이전 : 0.6028681993484497, 현재: 0.5960520505905151\n",
      "Epoch[13/1000] - Train loss: 0.58876 Train Accucracy: 0.83333 || Validation Loss: 0.58907 Validation Accuracy: 0.88112\n",
      "====================================================================================================\n",
      "저장: 13 - 이전 : 0.5960520505905151, 현재: 0.5890748500823975\n",
      "Epoch[14/1000] - Train loss: 0.58094 Train Accucracy: 0.84272 || Validation Loss: 0.58186 Validation Accuracy: 0.88112\n",
      "====================================================================================================\n",
      "저장: 14 - 이전 : 0.5890748500823975, 현재: 0.5818584263324738\n",
      "Epoch[15/1000] - Train loss: 0.56990 Train Accucracy: 0.85446 || Validation Loss: 0.57454 Validation Accuracy: 0.87413\n",
      "====================================================================================================\n",
      "저장: 15 - 이전 : 0.5818584263324738, 현재: 0.5745392143726349\n",
      "Epoch[16/1000] - Train loss: 0.56331 Train Accucracy: 0.86385 || Validation Loss: 0.56710 Validation Accuracy: 0.86713\n",
      "====================================================================================================\n",
      "저장: 16 - 이전 : 0.5745392143726349, 현재: 0.5671010613441467\n",
      "Epoch[17/1000] - Train loss: 0.55795 Train Accucracy: 0.85915 || Validation Loss: 0.55956 Validation Accuracy: 0.87413\n",
      "====================================================================================================\n",
      "저장: 17 - 이전 : 0.5671010613441467, 현재: 0.5595580637454987\n",
      "Epoch[18/1000] - Train loss: 0.54640 Train Accucracy: 0.86385 || Validation Loss: 0.55197 Validation Accuracy: 0.88811\n",
      "====================================================================================================\n",
      "저장: 18 - 이전 : 0.5595580637454987, 현재: 0.5519696474075317\n",
      "Epoch[19/1000] - Train loss: 0.53703 Train Accucracy: 0.87559 || Validation Loss: 0.54430 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 19 - 이전 : 0.5519696474075317, 현재: 0.5443021357059479\n",
      "Epoch[20/1000] - Train loss: 0.52980 Train Accucracy: 0.87089 || Validation Loss: 0.53657 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 20 - 이전 : 0.5443021357059479, 현재: 0.5365707576274872\n",
      "Epoch[21/1000] - Train loss: 0.52198 Train Accucracy: 0.87559 || Validation Loss: 0.52881 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 21 - 이전 : 0.5365707576274872, 현재: 0.5288129150867462\n",
      "Epoch[22/1000] - Train loss: 0.51333 Train Accucracy: 0.88028 || Validation Loss: 0.52093 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 22 - 이전 : 0.5288129150867462, 현재: 0.5209349393844604\n",
      "Epoch[23/1000] - Train loss: 0.50907 Train Accucracy: 0.87324 || Validation Loss: 0.51299 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 23 - 이전 : 0.5209349393844604, 현재: 0.5129949748516083\n",
      "Epoch[24/1000] - Train loss: 0.49820 Train Accucracy: 0.87089 || Validation Loss: 0.50504 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 24 - 이전 : 0.5129949748516083, 현재: 0.5050404369831085\n",
      "Epoch[25/1000] - Train loss: 0.48997 Train Accucracy: 0.87324 || Validation Loss: 0.49711 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 25 - 이전 : 0.5050404369831085, 현재: 0.4971130043268204\n",
      "Epoch[26/1000] - Train loss: 0.47829 Train Accucracy: 0.87793 || Validation Loss: 0.48921 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 26 - 이전 : 0.4971130043268204, 현재: 0.4892096370458603\n",
      "Epoch[27/1000] - Train loss: 0.47383 Train Accucracy: 0.87324 || Validation Loss: 0.48136 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 27 - 이전 : 0.4892096370458603, 현재: 0.4813646674156189\n",
      "Epoch[28/1000] - Train loss: 0.46332 Train Accucracy: 0.88028 || Validation Loss: 0.47361 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 28 - 이전 : 0.4813646674156189, 현재: 0.473605751991272\n",
      "Epoch[29/1000] - Train loss: 0.45309 Train Accucracy: 0.88732 || Validation Loss: 0.46590 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 29 - 이전 : 0.473605751991272, 현재: 0.4658986032009125\n",
      "Epoch[30/1000] - Train loss: 0.44316 Train Accucracy: 0.88028 || Validation Loss: 0.45823 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 30 - 이전 : 0.4658986032009125, 현재: 0.4582289159297943\n",
      "Epoch[31/1000] - Train loss: 0.43776 Train Accucracy: 0.88263 || Validation Loss: 0.45063 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 31 - 이전 : 0.4582289159297943, 현재: 0.4506312310695648\n",
      "Epoch[32/1000] - Train loss: 0.42911 Train Accucracy: 0.88498 || Validation Loss: 0.44306 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 32 - 이전 : 0.4506312310695648, 현재: 0.44305549561977386\n",
      "Epoch[33/1000] - Train loss: 0.42137 Train Accucracy: 0.88498 || Validation Loss: 0.43559 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 33 - 이전 : 0.44305549561977386, 현재: 0.4355914294719696\n",
      "Epoch[34/1000] - Train loss: 0.41281 Train Accucracy: 0.89202 || Validation Loss: 0.42813 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 34 - 이전 : 0.4355914294719696, 현재: 0.42812928557395935\n",
      "Epoch[35/1000] - Train loss: 0.40596 Train Accucracy: 0.88498 || Validation Loss: 0.42068 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 35 - 이전 : 0.42812928557395935, 현재: 0.4206800311803818\n",
      "Epoch[36/1000] - Train loss: 0.39818 Train Accucracy: 0.88967 || Validation Loss: 0.41330 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 36 - 이전 : 0.4206800311803818, 현재: 0.4132978916168213\n",
      "Epoch[37/1000] - Train loss: 0.38688 Train Accucracy: 0.88967 || Validation Loss: 0.40597 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 37 - 이전 : 0.4132978916168213, 현재: 0.4059731066226959\n",
      "Epoch[38/1000] - Train loss: 0.38502 Train Accucracy: 0.88498 || Validation Loss: 0.39855 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 38 - 이전 : 0.4059731066226959, 현재: 0.39855338633060455\n",
      "Epoch[39/1000] - Train loss: 0.37907 Train Accucracy: 0.88732 || Validation Loss: 0.39108 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 39 - 이전 : 0.39855338633060455, 현재: 0.3910837769508362\n",
      "Epoch[40/1000] - Train loss: 0.37236 Train Accucracy: 0.88732 || Validation Loss: 0.38358 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 40 - 이전 : 0.3910837769508362, 현재: 0.3835832178592682\n",
      "Epoch[41/1000] - Train loss: 0.36249 Train Accucracy: 0.88732 || Validation Loss: 0.37606 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 41 - 이전 : 0.3835832178592682, 현재: 0.3760598301887512\n",
      "Epoch[42/1000] - Train loss: 0.35157 Train Accucracy: 0.88967 || Validation Loss: 0.36860 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 42 - 이전 : 0.3760598301887512, 현재: 0.36859816312789917\n",
      "Epoch[43/1000] - Train loss: 0.34046 Train Accucracy: 0.89437 || Validation Loss: 0.36109 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 43 - 이전 : 0.36859816312789917, 현재: 0.36109021306037903\n",
      "Epoch[44/1000] - Train loss: 0.33753 Train Accucracy: 0.89202 || Validation Loss: 0.35359 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 44 - 이전 : 0.36109021306037903, 현재: 0.35358792543411255\n",
      "Epoch[45/1000] - Train loss: 0.32870 Train Accucracy: 0.89437 || Validation Loss: 0.34606 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 45 - 이전 : 0.35358792543411255, 현재: 0.34606318175792694\n",
      "Epoch[46/1000] - Train loss: 0.32085 Train Accucracy: 0.88967 || Validation Loss: 0.33848 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 46 - 이전 : 0.34606318175792694, 현재: 0.33847522735595703\n",
      "Epoch[47/1000] - Train loss: 0.31268 Train Accucracy: 0.89202 || Validation Loss: 0.33087 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 47 - 이전 : 0.33847522735595703, 현재: 0.33086565136909485\n",
      "Epoch[48/1000] - Train loss: 0.30507 Train Accucracy: 0.89202 || Validation Loss: 0.32328 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 48 - 이전 : 0.33086565136909485, 현재: 0.32327792048454285\n",
      "Epoch[49/1000] - Train loss: 0.29560 Train Accucracy: 0.89906 || Validation Loss: 0.31570 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 49 - 이전 : 0.32327792048454285, 현재: 0.3156997561454773\n",
      "Epoch[50/1000] - Train loss: 0.28908 Train Accucracy: 0.89437 || Validation Loss: 0.30814 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 50 - 이전 : 0.3156997561454773, 현재: 0.3081442713737488\n",
      "Epoch[51/1000] - Train loss: 0.27966 Train Accucracy: 0.89437 || Validation Loss: 0.30058 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 51 - 이전 : 0.3081442713737488, 현재: 0.30057644844055176\n",
      "Epoch[52/1000] - Train loss: 0.27652 Train Accucracy: 0.89671 || Validation Loss: 0.29303 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 52 - 이전 : 0.30057644844055176, 현재: 0.29303064942359924\n",
      "Epoch[53/1000] - Train loss: 0.26753 Train Accucracy: 0.89906 || Validation Loss: 0.28557 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 53 - 이전 : 0.29303064942359924, 현재: 0.285568967461586\n",
      "Epoch[54/1000] - Train loss: 0.25770 Train Accucracy: 0.89906 || Validation Loss: 0.27822 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 54 - 이전 : 0.285568967461586, 현재: 0.278215691447258\n",
      "Epoch[55/1000] - Train loss: 0.24968 Train Accucracy: 0.90141 || Validation Loss: 0.27092 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 55 - 이전 : 0.278215691447258, 현재: 0.2709241658449173\n",
      "Epoch[56/1000] - Train loss: 0.24534 Train Accucracy: 0.89906 || Validation Loss: 0.26373 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 56 - 이전 : 0.2709241658449173, 현재: 0.263726606965065\n",
      "Epoch[57/1000] - Train loss: 0.23311 Train Accucracy: 0.90141 || Validation Loss: 0.25662 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 57 - 이전 : 0.263726606965065, 현재: 0.2566225826740265\n",
      "Epoch[58/1000] - Train loss: 0.22693 Train Accucracy: 0.89906 || Validation Loss: 0.24965 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 58 - 이전 : 0.2566225826740265, 현재: 0.24965448677539825\n",
      "Epoch[59/1000] - Train loss: 0.21728 Train Accucracy: 0.90141 || Validation Loss: 0.24281 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 59 - 이전 : 0.24965448677539825, 현재: 0.2428058162331581\n",
      "Epoch[60/1000] - Train loss: 0.21309 Train Accucracy: 0.90376 || Validation Loss: 0.23618 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 60 - 이전 : 0.2428058162331581, 현재: 0.23618192225694656\n",
      "Epoch[61/1000] - Train loss: 0.20649 Train Accucracy: 0.90610 || Validation Loss: 0.22974 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 61 - 이전 : 0.23618192225694656, 현재: 0.22973959892988205\n",
      "Epoch[62/1000] - Train loss: 0.19879 Train Accucracy: 0.90610 || Validation Loss: 0.22348 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 62 - 이전 : 0.22973959892988205, 현재: 0.22347941994667053\n",
      "Epoch[63/1000] - Train loss: 0.18618 Train Accucracy: 0.91315 || Validation Loss: 0.21745 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 63 - 이전 : 0.22347941994667053, 현재: 0.21745016425848007\n",
      "Epoch[64/1000] - Train loss: 0.18584 Train Accucracy: 0.90845 || Validation Loss: 0.21166 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 64 - 이전 : 0.21745016425848007, 현재: 0.21165647357702255\n",
      "Epoch[65/1000] - Train loss: 0.17960 Train Accucracy: 0.90845 || Validation Loss: 0.20612 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 65 - 이전 : 0.21165647357702255, 현재: 0.20611677318811417\n",
      "Epoch[66/1000] - Train loss: 0.17577 Train Accucracy: 0.90845 || Validation Loss: 0.20082 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 66 - 이전 : 0.20611677318811417, 현재: 0.20081771910190582\n",
      "Epoch[67/1000] - Train loss: 0.17268 Train Accucracy: 0.90845 || Validation Loss: 0.19580 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 67 - 이전 : 0.20081771910190582, 현재: 0.19580385088920593\n",
      "Epoch[68/1000] - Train loss: 0.15464 Train Accucracy: 0.92019 || Validation Loss: 0.19099 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 68 - 이전 : 0.19580385088920593, 현재: 0.19099438190460205\n",
      "Epoch[69/1000] - Train loss: 0.15392 Train Accucracy: 0.91315 || Validation Loss: 0.18644 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 69 - 이전 : 0.19099438190460205, 현재: 0.18643509596586227\n",
      "Epoch[70/1000] - Train loss: 0.15206 Train Accucracy: 0.91315 || Validation Loss: 0.18211 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 70 - 이전 : 0.18643509596586227, 현재: 0.1821053922176361\n",
      "Epoch[71/1000] - Train loss: 0.14781 Train Accucracy: 0.91315 || Validation Loss: 0.17804 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 71 - 이전 : 0.1821053922176361, 현재: 0.17804346233606339\n",
      "Epoch[72/1000] - Train loss: 0.14259 Train Accucracy: 0.91315 || Validation Loss: 0.17418 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 72 - 이전 : 0.17804346233606339, 현재: 0.17417847365140915\n",
      "Epoch[73/1000] - Train loss: 0.13588 Train Accucracy: 0.91549 || Validation Loss: 0.17051 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 73 - 이전 : 0.17417847365140915, 현재: 0.17050647735595703\n",
      "Epoch[74/1000] - Train loss: 0.13179 Train Accucracy: 0.91784 || Validation Loss: 0.16704 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 74 - 이전 : 0.17050647735595703, 현재: 0.16704459488391876\n",
      "Epoch[75/1000] - Train loss: 0.13079 Train Accucracy: 0.91784 || Validation Loss: 0.16380 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 75 - 이전 : 0.16704459488391876, 현재: 0.16379738599061966\n",
      "Epoch[76/1000] - Train loss: 0.12674 Train Accucracy: 0.91784 || Validation Loss: 0.16075 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 76 - 이전 : 0.16379738599061966, 현재: 0.16074901074171066\n",
      "Epoch[77/1000] - Train loss: 0.12376 Train Accucracy: 0.91784 || Validation Loss: 0.15788 Validation Accuracy: 0.95105\n",
      "====================================================================================================\n",
      "저장: 77 - 이전 : 0.16074901074171066, 현재: 0.15787801146507263\n",
      "Epoch[78/1000] - Train loss: 0.11829 Train Accucracy: 0.92019 || Validation Loss: 0.15520 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 78 - 이전 : 0.15787801146507263, 현재: 0.15520308911800385\n",
      "Epoch[79/1000] - Train loss: 0.12040 Train Accucracy: 0.92019 || Validation Loss: 0.15266 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 79 - 이전 : 0.15520308911800385, 현재: 0.15266097337007523\n",
      "Epoch[80/1000] - Train loss: 0.11351 Train Accucracy: 0.92019 || Validation Loss: 0.15028 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 80 - 이전 : 0.15266097337007523, 현재: 0.1502816528081894\n",
      "Epoch[81/1000] - Train loss: 0.11283 Train Accucracy: 0.92019 || Validation Loss: 0.14802 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 81 - 이전 : 0.1502816528081894, 현재: 0.14802107959985733\n",
      "Epoch[82/1000] - Train loss: 0.10925 Train Accucracy: 0.92019 || Validation Loss: 0.14596 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 82 - 이전 : 0.14802107959985733, 현재: 0.1459566056728363\n",
      "Epoch[83/1000] - Train loss: 0.10713 Train Accucracy: 0.92019 || Validation Loss: 0.14396 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 83 - 이전 : 0.1459566056728363, 현재: 0.14395949244499207\n",
      "Epoch[84/1000] - Train loss: 0.10107 Train Accucracy: 0.92488 || Validation Loss: 0.14209 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 84 - 이전 : 0.14395949244499207, 현재: 0.14209040999412537\n",
      "Epoch[85/1000] - Train loss: 0.09626 Train Accucracy: 0.92488 || Validation Loss: 0.14037 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 85 - 이전 : 0.14209040999412537, 현재: 0.14036673307418823\n",
      "Epoch[86/1000] - Train loss: 0.09561 Train Accucracy: 0.92488 || Validation Loss: 0.13874 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 86 - 이전 : 0.14036673307418823, 현재: 0.138737253844738\n",
      "Epoch[87/1000] - Train loss: 0.09745 Train Accucracy: 0.92254 || Validation Loss: 0.13717 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 87 - 이전 : 0.138737253844738, 현재: 0.1371692232787609\n",
      "Epoch[88/1000] - Train loss: 0.09452 Train Accucracy: 0.92254 || Validation Loss: 0.13569 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 88 - 이전 : 0.1371692232787609, 현재: 0.13568735495209694\n",
      "Epoch[89/1000] - Train loss: 0.09271 Train Accucracy: 0.92488 || Validation Loss: 0.13434 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 89 - 이전 : 0.13568735495209694, 현재: 0.1343393437564373\n",
      "Epoch[90/1000] - Train loss: 0.09037 Train Accucracy: 0.92254 || Validation Loss: 0.13310 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 90 - 이전 : 0.1343393437564373, 현재: 0.13309738785028458\n",
      "Epoch[91/1000] - Train loss: 0.09125 Train Accucracy: 0.92254 || Validation Loss: 0.13189 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 91 - 이전 : 0.13309738785028458, 현재: 0.1318906657397747\n",
      "Epoch[92/1000] - Train loss: 0.08897 Train Accucracy: 0.92488 || Validation Loss: 0.13073 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 92 - 이전 : 0.1318906657397747, 현재: 0.13073453679680824\n",
      "Epoch[93/1000] - Train loss: 0.08554 Train Accucracy: 0.92488 || Validation Loss: 0.12965 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 93 - 이전 : 0.13073453679680824, 현재: 0.12964721396565437\n",
      "Epoch[94/1000] - Train loss: 0.08643 Train Accucracy: 0.92488 || Validation Loss: 0.12866 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 94 - 이전 : 0.12964721396565437, 현재: 0.12865541502833366\n",
      "Epoch[95/1000] - Train loss: 0.08540 Train Accucracy: 0.92488 || Validation Loss: 0.12768 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 95 - 이전 : 0.12865541502833366, 현재: 0.12768126279115677\n",
      "Epoch[96/1000] - Train loss: 0.07663 Train Accucracy: 0.92488 || Validation Loss: 0.12679 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 96 - 이전 : 0.12768126279115677, 현재: 0.126789640635252\n",
      "Epoch[97/1000] - Train loss: 0.08164 Train Accucracy: 0.92254 || Validation Loss: 0.12594 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 97 - 이전 : 0.126789640635252, 현재: 0.1259387731552124\n",
      "Epoch[98/1000] - Train loss: 0.07720 Train Accucracy: 0.92488 || Validation Loss: 0.12516 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 98 - 이전 : 0.1259387731552124, 현재: 0.12515704333782196\n",
      "Epoch[99/1000] - Train loss: 0.07211 Train Accucracy: 0.92488 || Validation Loss: 0.12442 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 99 - 이전 : 0.12515704333782196, 현재: 0.12441611662507057\n",
      "Epoch[100/1000] - Train loss: 0.07864 Train Accucracy: 0.92254 || Validation Loss: 0.12369 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 100 - 이전 : 0.12441611662507057, 현재: 0.12369301170110703\n",
      "Epoch[101/1000] - Train loss: 0.06894 Train Accucracy: 0.92723 || Validation Loss: 0.12301 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 101 - 이전 : 0.12369301170110703, 현재: 0.12300607934594154\n",
      "Epoch[102/1000] - Train loss: 0.07519 Train Accucracy: 0.92254 || Validation Loss: 0.12232 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 102 - 이전 : 0.12300607934594154, 현재: 0.12232282757759094\n",
      "Epoch[103/1000] - Train loss: 0.06853 Train Accucracy: 0.92488 || Validation Loss: 0.12168 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 103 - 이전 : 0.12232282757759094, 현재: 0.12167592346668243\n",
      "Epoch[104/1000] - Train loss: 0.06835 Train Accucracy: 0.92488 || Validation Loss: 0.12108 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 104 - 이전 : 0.12167592346668243, 현재: 0.12107901275157928\n",
      "Epoch[105/1000] - Train loss: 0.07234 Train Accucracy: 0.92488 || Validation Loss: 0.12052 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 105 - 이전 : 0.12107901275157928, 현재: 0.12052207067608833\n",
      "Epoch[106/1000] - Train loss: 0.06718 Train Accucracy: 0.92723 || Validation Loss: 0.12003 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 106 - 이전 : 0.12052207067608833, 현재: 0.12003033980727196\n",
      "Epoch[107/1000] - Train loss: 0.06989 Train Accucracy: 0.92488 || Validation Loss: 0.11957 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 107 - 이전 : 0.12003033980727196, 현재: 0.11956895515322685\n",
      "Epoch[108/1000] - Train loss: 0.06938 Train Accucracy: 0.92488 || Validation Loss: 0.11911 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 108 - 이전 : 0.11956895515322685, 현재: 0.11911033093929291\n",
      "Epoch[109/1000] - Train loss: 0.06975 Train Accucracy: 0.92488 || Validation Loss: 0.11872 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 109 - 이전 : 0.11911033093929291, 현재: 0.11871567368507385\n",
      "Epoch[110/1000] - Train loss: 0.06501 Train Accucracy: 0.92723 || Validation Loss: 0.11827 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 110 - 이전 : 0.11871567368507385, 현재: 0.11827287450432777\n",
      "Epoch[111/1000] - Train loss: 0.06684 Train Accucracy: 0.92488 || Validation Loss: 0.11791 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 111 - 이전 : 0.11827287450432777, 현재: 0.1179063692688942\n",
      "Epoch[112/1000] - Train loss: 0.06262 Train Accucracy: 0.92723 || Validation Loss: 0.11754 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 112 - 이전 : 0.1179063692688942, 현재: 0.1175423227250576\n",
      "Epoch[113/1000] - Train loss: 0.06215 Train Accucracy: 0.92958 || Validation Loss: 0.11721 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 113 - 이전 : 0.1175423227250576, 현재: 0.11720946431159973\n",
      "Epoch[114/1000] - Train loss: 0.06606 Train Accucracy: 0.92488 || Validation Loss: 0.11690 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 114 - 이전 : 0.11720946431159973, 현재: 0.11689501628279686\n",
      "Epoch[115/1000] - Train loss: 0.06537 Train Accucracy: 0.92488 || Validation Loss: 0.11662 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 115 - 이전 : 0.11689501628279686, 현재: 0.11661602929234505\n",
      "Epoch[116/1000] - Train loss: 0.06428 Train Accucracy: 0.92488 || Validation Loss: 0.11632 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 116 - 이전 : 0.11661602929234505, 현재: 0.11631537973880768\n",
      "Epoch[117/1000] - Train loss: 0.06194 Train Accucracy: 0.92488 || Validation Loss: 0.11602 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 117 - 이전 : 0.11631537973880768, 현재: 0.11602160334587097\n",
      "Epoch[118/1000] - Train loss: 0.06061 Train Accucracy: 0.92488 || Validation Loss: 0.11576 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 118 - 이전 : 0.11602160334587097, 현재: 0.11576096713542938\n",
      "Epoch[119/1000] - Train loss: 0.05930 Train Accucracy: 0.92488 || Validation Loss: 0.11548 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 119 - 이전 : 0.11576096713542938, 현재: 0.11548391729593277\n",
      "Epoch[120/1000] - Train loss: 0.05951 Train Accucracy: 0.92488 || Validation Loss: 0.11525 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 120 - 이전 : 0.11548391729593277, 현재: 0.11524758860468864\n",
      "Epoch[121/1000] - Train loss: 0.05913 Train Accucracy: 0.92488 || Validation Loss: 0.11501 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 121 - 이전 : 0.11524758860468864, 현재: 0.1150083988904953\n",
      "Epoch[122/1000] - Train loss: 0.05530 Train Accucracy: 0.92723 || Validation Loss: 0.11476 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 122 - 이전 : 0.1150083988904953, 현재: 0.11475641280412674\n",
      "Epoch[123/1000] - Train loss: 0.05906 Train Accucracy: 0.92488 || Validation Loss: 0.11457 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 123 - 이전 : 0.11475641280412674, 현재: 0.1145675778388977\n",
      "Epoch[124/1000] - Train loss: 0.05865 Train Accucracy: 0.92488 || Validation Loss: 0.11437 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 124 - 이전 : 0.1145675778388977, 현재: 0.11436765640974045\n",
      "Epoch[125/1000] - Train loss: 0.04129 Train Accucracy: 0.93192 || Validation Loss: 0.11415 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 125 - 이전 : 0.11436765640974045, 현재: 0.11414699256420135\n",
      "Epoch[126/1000] - Train loss: 0.05733 Train Accucracy: 0.92488 || Validation Loss: 0.11398 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 126 - 이전 : 0.11414699256420135, 현재: 0.1139790527522564\n",
      "Epoch[127/1000] - Train loss: 0.05651 Train Accucracy: 0.92488 || Validation Loss: 0.11384 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 127 - 이전 : 0.1139790527522564, 현재: 0.11383594200015068\n",
      "Epoch[128/1000] - Train loss: 0.05521 Train Accucracy: 0.92488 || Validation Loss: 0.11370 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 128 - 이전 : 0.11383594200015068, 현재: 0.11370015889406204\n",
      "Epoch[129/1000] - Train loss: 0.05521 Train Accucracy: 0.92488 || Validation Loss: 0.11366 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 129 - 이전 : 0.11370015889406204, 현재: 0.11365792900323868\n",
      "Epoch[130/1000] - Train loss: 0.04435 Train Accucracy: 0.92723 || Validation Loss: 0.11360 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 130 - 이전 : 0.11365792900323868, 현재: 0.11359665542840958\n",
      "Epoch[131/1000] - Train loss: 0.05159 Train Accucracy: 0.92723 || Validation Loss: 0.11356 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 131 - 이전 : 0.11359665542840958, 현재: 0.1135624386370182\n",
      "Epoch[132/1000] - Train loss: 0.05167 Train Accucracy: 0.92958 || Validation Loss: 0.11348 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 132 - 이전 : 0.1135624386370182, 현재: 0.11348192766308784\n",
      "Epoch[133/1000] - Train loss: 0.05061 Train Accucracy: 0.92958 || Validation Loss: 0.11344 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 133 - 이전 : 0.11348192766308784, 현재: 0.11343623697757721\n",
      "Epoch[134/1000] - Train loss: 0.05150 Train Accucracy: 0.92723 || Validation Loss: 0.11340 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 134 - 이전 : 0.11343623697757721, 현재: 0.1133989691734314\n",
      "Epoch[135/1000] - Train loss: 0.04839 Train Accucracy: 0.92958 || Validation Loss: 0.11333 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 135 - 이전 : 0.1133989691734314, 현재: 0.11333028972148895\n",
      "Epoch[136/1000] - Train loss: 0.05341 Train Accucracy: 0.92723 || Validation Loss: 0.11337 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[137/1000] - Train loss: 0.05135 Train Accucracy: 0.92723 || Validation Loss: 0.11339 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[138/1000] - Train loss: 0.04908 Train Accucracy: 0.92958 || Validation Loss: 0.11338 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[139/1000] - Train loss: 0.04009 Train Accucracy: 0.92958 || Validation Loss: 0.11331 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 139 - 이전 : 0.11333028972148895, 현재: 0.11330581083893776\n",
      "Epoch[140/1000] - Train loss: 0.04552 Train Accucracy: 0.93192 || Validation Loss: 0.11329 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 140 - 이전 : 0.11330581083893776, 현재: 0.11328664049506187\n",
      "Epoch[141/1000] - Train loss: 0.04971 Train Accucracy: 0.92723 || Validation Loss: 0.11326 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 141 - 이전 : 0.11328664049506187, 현재: 0.11325590685009956\n",
      "Epoch[142/1000] - Train loss: 0.04864 Train Accucracy: 0.92723 || Validation Loss: 0.11326 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[143/1000] - Train loss: 0.04725 Train Accucracy: 0.92958 || Validation Loss: 0.11328 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[144/1000] - Train loss: 0.04749 Train Accucracy: 0.92723 || Validation Loss: 0.11327 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "Epoch[145/1000] - Train loss: 0.04821 Train Accucracy: 0.92723 || Validation Loss: 0.11318 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 145 - 이전 : 0.11325590685009956, 현재: 0.11318263411521912\n",
      "Epoch[146/1000] - Train loss: 0.03651 Train Accucracy: 0.92958 || Validation Loss: 0.11315 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 146 - 이전 : 0.11318263411521912, 현재: 0.11315110698342323\n",
      "Epoch[147/1000] - Train loss: 0.04684 Train Accucracy: 0.92723 || Validation Loss: 0.11314 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 147 - 이전 : 0.11315110698342323, 현재: 0.11313853412866592\n",
      "Epoch[148/1000] - Train loss: 0.04773 Train Accucracy: 0.92723 || Validation Loss: 0.11308 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 148 - 이전 : 0.11313853412866592, 현재: 0.11307759955525398\n",
      "Epoch[149/1000] - Train loss: 0.04209 Train Accucracy: 0.93192 || Validation Loss: 0.11299 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 149 - 이전 : 0.11307759955525398, 현재: 0.1129889041185379\n",
      "Epoch[150/1000] - Train loss: 0.04703 Train Accucracy: 0.92723 || Validation Loss: 0.11291 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 150 - 이전 : 0.1129889041185379, 현재: 0.11291499435901642\n",
      "Epoch[151/1000] - Train loss: 0.04023 Train Accucracy: 0.93192 || Validation Loss: 0.11288 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 151 - 이전 : 0.11291499435901642, 현재: 0.11287714168429375\n",
      "Epoch[152/1000] - Train loss: 0.04450 Train Accucracy: 0.92958 || Validation Loss: 0.11279 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 152 - 이전 : 0.11287714168429375, 현재: 0.11279449611902237\n",
      "Epoch[153/1000] - Train loss: 0.04483 Train Accucracy: 0.92958 || Validation Loss: 0.11268 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 153 - 이전 : 0.11279449611902237, 현재: 0.1126784235239029\n",
      "Epoch[154/1000] - Train loss: 0.04498 Train Accucracy: 0.92958 || Validation Loss: 0.11257 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 154 - 이전 : 0.1126784235239029, 현재: 0.11256671324372292\n",
      "Epoch[155/1000] - Train loss: 0.04570 Train Accucracy: 0.92723 || Validation Loss: 0.11254 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 155 - 이전 : 0.11256671324372292, 현재: 0.11253519728779793\n",
      "Epoch[156/1000] - Train loss: 0.04443 Train Accucracy: 0.92958 || Validation Loss: 0.11233 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 156 - 이전 : 0.11253519728779793, 현재: 0.11232653632760048\n",
      "Epoch[157/1000] - Train loss: 0.04007 Train Accucracy: 0.93192 || Validation Loss: 0.11210 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 157 - 이전 : 0.11232653632760048, 현재: 0.11209819838404655\n",
      "Epoch[158/1000] - Train loss: 0.04402 Train Accucracy: 0.92958 || Validation Loss: 0.11201 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 158 - 이전 : 0.11209819838404655, 현재: 0.11201082170009613\n",
      "Epoch[159/1000] - Train loss: 0.04420 Train Accucracy: 0.92958 || Validation Loss: 0.11183 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 159 - 이전 : 0.11201082170009613, 현재: 0.11183023825287819\n",
      "Epoch[160/1000] - Train loss: 0.04318 Train Accucracy: 0.92958 || Validation Loss: 0.11171 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 160 - 이전 : 0.11183023825287819, 현재: 0.11170743778347969\n",
      "Epoch[161/1000] - Train loss: 0.03012 Train Accucracy: 0.93192 || Validation Loss: 0.11159 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 161 - 이전 : 0.11170743778347969, 현재: 0.11159364134073257\n",
      "Epoch[162/1000] - Train loss: 0.04348 Train Accucracy: 0.92958 || Validation Loss: 0.11154 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 162 - 이전 : 0.11159364134073257, 현재: 0.11154255643486977\n",
      "Epoch[163/1000] - Train loss: 0.03965 Train Accucracy: 0.93192 || Validation Loss: 0.11148 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 163 - 이전 : 0.11154255643486977, 현재: 0.11148347333073616\n",
      "Epoch[164/1000] - Train loss: 0.04235 Train Accucracy: 0.92958 || Validation Loss: 0.11149 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "Epoch[165/1000] - Train loss: 0.03785 Train Accucracy: 0.93427 || Validation Loss: 0.11148 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 165 - 이전 : 0.11148347333073616, 현재: 0.111478541046381\n",
      "Epoch[166/1000] - Train loss: 0.04146 Train Accucracy: 0.92958 || Validation Loss: 0.11145 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 166 - 이전 : 0.111478541046381, 현재: 0.11145344749093056\n",
      "Epoch[167/1000] - Train loss: 0.04098 Train Accucracy: 0.92958 || Validation Loss: 0.11145 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 167 - 이전 : 0.11145344749093056, 현재: 0.11144609749317169\n",
      "Epoch[168/1000] - Train loss: 0.03932 Train Accucracy: 0.92958 || Validation Loss: 0.11156 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "Epoch[169/1000] - Train loss: 0.04144 Train Accucracy: 0.92958 || Validation Loss: 0.11164 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "Epoch[170/1000] - Train loss: 0.04108 Train Accucracy: 0.92958 || Validation Loss: 0.11165 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "Epoch[171/1000] - Train loss: 0.04002 Train Accucracy: 0.92958 || Validation Loss: 0.11170 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "Epoch[172/1000] - Train loss: 0.04031 Train Accucracy: 0.92958 || Validation Loss: 0.11163 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[173/1000] - Train loss: 0.03641 Train Accucracy: 0.92958 || Validation Loss: 0.11162 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[174/1000] - Train loss: 0.03992 Train Accucracy: 0.92958 || Validation Loss: 0.11160 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[175/1000] - Train loss: 0.03837 Train Accucracy: 0.92958 || Validation Loss: 0.11166 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[176/1000] - Train loss: 0.03744 Train Accucracy: 0.92958 || Validation Loss: 0.11153 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[177/1000] - Train loss: 0.03914 Train Accucracy: 0.92958 || Validation Loss: 0.11137 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 177 - 이전 : 0.11144609749317169, 현재: 0.11137315258383751\n",
      "Epoch[178/1000] - Train loss: 0.03876 Train Accucracy: 0.92958 || Validation Loss: 0.11121 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 178 - 이전 : 0.11137315258383751, 현재: 0.11121369898319244\n",
      "Epoch[179/1000] - Train loss: 0.03782 Train Accucracy: 0.92958 || Validation Loss: 0.11109 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 179 - 이전 : 0.11121369898319244, 현재: 0.11108719930052757\n",
      "Epoch[180/1000] - Train loss: 0.03292 Train Accucracy: 0.93427 || Validation Loss: 0.11095 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 180 - 이전 : 0.11108719930052757, 현재: 0.1109493225812912\n",
      "Epoch[181/1000] - Train loss: 0.03822 Train Accucracy: 0.92958 || Validation Loss: 0.11075 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 181 - 이전 : 0.1109493225812912, 현재: 0.11074932664632797\n",
      "Epoch[182/1000] - Train loss: 0.03600 Train Accucracy: 0.92958 || Validation Loss: 0.11063 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 182 - 이전 : 0.11074932664632797, 현재: 0.11063352599740028\n",
      "Epoch[183/1000] - Train loss: 0.03840 Train Accucracy: 0.92958 || Validation Loss: 0.11049 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 183 - 이전 : 0.11063352599740028, 현재: 0.11049496382474899\n",
      "Epoch[184/1000] - Train loss: 0.03395 Train Accucracy: 0.93192 || Validation Loss: 0.11035 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 184 - 이전 : 0.11049496382474899, 현재: 0.11035121232271194\n",
      "Epoch[185/1000] - Train loss: 0.03470 Train Accucracy: 0.92958 || Validation Loss: 0.11030 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 185 - 이전 : 0.11035121232271194, 현재: 0.11029700934886932\n",
      "Epoch[186/1000] - Train loss: 0.03066 Train Accucracy: 0.93192 || Validation Loss: 0.11017 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 186 - 이전 : 0.11029700934886932, 현재: 0.11017410829663277\n",
      "Epoch[187/1000] - Train loss: 0.03708 Train Accucracy: 0.92958 || Validation Loss: 0.11010 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 187 - 이전 : 0.11017410829663277, 현재: 0.11010218784213066\n",
      "Epoch[188/1000] - Train loss: 0.03416 Train Accucracy: 0.93192 || Validation Loss: 0.11005 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 188 - 이전 : 0.11010218784213066, 현재: 0.11005085334181786\n",
      "Epoch[189/1000] - Train loss: 0.03695 Train Accucracy: 0.92958 || Validation Loss: 0.11001 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 189 - 이전 : 0.11005085334181786, 현재: 0.11001135408878326\n",
      "Epoch[190/1000] - Train loss: 0.03324 Train Accucracy: 0.93192 || Validation Loss: 0.10992 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 190 - 이전 : 0.11001135408878326, 현재: 0.10991623625159264\n",
      "Epoch[191/1000] - Train loss: 0.02383 Train Accucracy: 0.93192 || Validation Loss: 0.10985 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 191 - 이전 : 0.10991623625159264, 현재: 0.10984713211655617\n",
      "Epoch[192/1000] - Train loss: 0.03569 Train Accucracy: 0.92958 || Validation Loss: 0.10976 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 192 - 이전 : 0.10984713211655617, 현재: 0.10976061597466469\n",
      "Epoch[193/1000] - Train loss: 0.03541 Train Accucracy: 0.92958 || Validation Loss: 0.10976 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[194/1000] - Train loss: 0.03248 Train Accucracy: 0.93427 || Validation Loss: 0.10980 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[195/1000] - Train loss: 0.03403 Train Accucracy: 0.93192 || Validation Loss: 0.10992 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[196/1000] - Train loss: 0.03443 Train Accucracy: 0.93192 || Validation Loss: 0.11003 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[197/1000] - Train loss: 0.03440 Train Accucracy: 0.93192 || Validation Loss: 0.11007 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[198/1000] - Train loss: 0.03414 Train Accucracy: 0.93192 || Validation Loss: 0.11013 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[199/1000] - Train loss: 0.03384 Train Accucracy: 0.93192 || Validation Loss: 0.11014 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[200/1000] - Train loss: 0.03475 Train Accucracy: 0.93192 || Validation Loss: 0.11012 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[201/1000] - Train loss: 0.03382 Train Accucracy: 0.93192 || Validation Loss: 0.11015 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Epoch[202/1000] - Train loss: 0.03109 Train Accucracy: 0.93427 || Validation Loss: 0.11025 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "Early stopping: Epoch - 201\n",
      "1.4540033340454102 초\n"
     ]
    }
   ],
   "source": [
    "from module.train import fit\n",
    "\n",
    "result = fit(\n",
    "    train_loader,test_loader,model,loss_fn,optimizer,epochs=1000,\n",
    "    save_best_model = True,save_model_path ='saved_models/bc_mode12.pth',\n",
    "    device=device , mode='binary' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심 특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
